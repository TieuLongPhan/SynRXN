{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synrxn.data_loader import DataLoader\n",
    "dl = DataLoader(task=\"property\")\n",
    "dl.print_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synrxn.split.repeated_kfold import RepeatedKFoldsSplitter\n",
    "df = dl.load(\"b97xd3\")\n",
    "splitter = RepeatedKFoldsSplitter(\n",
    "            n_splits=5,\n",
    "            n_repeats=5,\n",
    "            ratio=(8, 1, 1),\n",
    "            shuffle=True,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "splitter.split(df, stratify_col=None)\n",
    "\n",
    "train_df, val_df, test_df = splitter.get_split(repeat=0, fold=0, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "import hashlib\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "_GH_RAW_TPL = \"https://raw.githubusercontent.com/{owner}/{repo}/refs/{ref_type}/{ref}/Data\"\n",
    "_GH_API_TPL = \"https://api.github.com/repos/{owner}/{repo}/contents/Data/{task}?ref={ref}\"\n",
    "_ZENODO_API_TPL = \"https://zenodo.org/api/records/{record_id}\"\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        task: str,\n",
    "        source: str = \"github\",                 # \"github\" or \"zenodo\"\n",
    "        owner: str = \"TieuLongPhan\",\n",
    "        repo: str = \"SynRXN\",\n",
    "        ref: str = \"main\",                      # branch, tag, or commit SHA (when source=\"github\")\n",
    "        ref_type: str = \"heads\",                # \"heads\" for branches/SHAs, \"tags\" for releases\n",
    "        zenodo_record: Optional[int] = None,    # e.g. 17297723\n",
    "        zenodo_doi: Optional[str] = None,       # e.g. \"10.5281/zenodo.17297723\" (auto-parsed)\n",
    "        cache_dir: Optional[Path] = None,\n",
    "        timeout: int = 20,\n",
    "        user_agent: str = \"SynRXN-DataLoader/1.0\",\n",
    "        max_workers: int = 6,\n",
    "    ) -> None:\n",
    "        self.task = str(task).strip(\"/\")\n",
    "\n",
    "        self.source = source.lower().strip()\n",
    "        if self.source not in {\"github\", \"zenodo\"}:\n",
    "            raise ValueError(\"source must be 'github' or 'zenodo'\")\n",
    "\n",
    "        # GitHub config\n",
    "        self.owner = owner\n",
    "        self.repo = repo\n",
    "        self.ref = ref\n",
    "        self.ref_type = ref_type  # \"heads\" or \"tags\"\n",
    "        self._gh_raw_base = _GH_RAW_TPL.format(owner=self.owner, repo=self.repo, ref_type=self.ref_type, ref=self.ref)\n",
    "        self._gh_api_url = _GH_API_TPL.format(owner=self.owner, repo=self.repo, task=self.task, ref=self.ref)\n",
    "\n",
    "        # Zenodo config\n",
    "        self.zenodo_record = self._infer_zenodo_record(zenodo_record, zenodo_doi)\n",
    "\n",
    "        # IO / network\n",
    "        self.timeout = int(timeout)\n",
    "        self.headers = {\"User-Agent\": user_agent}\n",
    "        self.max_workers = int(max_workers)\n",
    "\n",
    "        # cache\n",
    "        self.cache_dir: Optional[Path] = Path(cache_dir).expanduser().resolve() if cache_dir else None\n",
    "        if self.cache_dir:\n",
    "            self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # memo\n",
    "        self._names_cache: Optional[List[str]] = None\n",
    "        self._zenodo_file_index: Optional[Dict[str, Dict]] = None  # maps file key -> file meta (from zenodo)\n",
    "\n",
    "    # ------------- OOP niceties -------------\n",
    "    def __repr__(self) -> str:\n",
    "        z = self.zenodo_record if self.zenodo_record is not None else None\n",
    "        return (f\"DataLoader(task={self.task!r}, source={self.source!r}, \"\n",
    "                f\"github={self.owner}/{self.repo}@{self.ref_type}/{self.ref}, \"\n",
    "                f\"zenodo_record={z}, cache_dir={str(self.cache_dir) if self.cache_dir else None})\")\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self.source == \"github\":\n",
    "            return f\"<DataLoader github:{self.owner}/{self.repo}@{self.ref_type}/{self.ref} task={self.task}>\"\n",
    "        else:\n",
    "            return f\"<DataLoader zenodo:{self.zenodo_record} task={self.task}>\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.names)\n",
    "\n",
    "    def __contains__(self, name: str) -> bool:\n",
    "        return name in self.names\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.names\n",
    "\n",
    "    # ------------- Properties -------------\n",
    "    @property\n",
    "    def names(self) -> List[str]:\n",
    "        return self.available_names()\n",
    "\n",
    "    @property\n",
    "    def raw_base(self) -> str:\n",
    "        return self._gh_raw_base\n",
    "\n",
    "    @property\n",
    "    def api_url(self) -> str:\n",
    "        return self._gh_api_url\n",
    "\n",
    "    # ------------- Public API -------------\n",
    "    def available_names(self, refresh: bool = False) -> List[str]:\n",
    "        if self._names_cache is not None and not refresh:\n",
    "            return list(self._names_cache)\n",
    "\n",
    "        if self.source == \"github\":\n",
    "            names = self._fetch_names_github()\n",
    "        else:\n",
    "            names = self._fetch_names_zenodo()\n",
    "\n",
    "        self._names_cache = sorted(set(names))\n",
    "        return list(self._names_cache)\n",
    "\n",
    "    def refresh_names(self) -> List[str]:\n",
    "        return self.available_names(refresh=True)\n",
    "\n",
    "    def suggest(self, name: str, n: int = 5) -> List[str]:\n",
    "        import difflib\n",
    "        names = self.available_names()\n",
    "        if not names:\n",
    "            return []\n",
    "        return difflib.get_close_matches(name, names, n=n, cutoff=0.4)\n",
    "\n",
    "    def print_names(self, cols: int = 3, show_count: bool = True) -> None:\n",
    "        names = self.available_names()\n",
    "        if show_count:\n",
    "            print(f\"Datasets in task '{self.task}': {len(names)}\")\n",
    "        if not names:\n",
    "            print(\"  (no names found)\")\n",
    "            return\n",
    "        rows = math.ceil(len(names) / cols)\n",
    "        padded = names + [\"\"] * (rows * cols - len(names))\n",
    "        matrix = [padded[i : i + rows] for i in range(0, rows * cols, rows)]\n",
    "        for r in range(rows):\n",
    "            row_items = [matrix[c][r].ljust(30) for c in range(cols) if matrix[c][r]]\n",
    "            print(\"  \" + \"  \".join(row_items))\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        name: str,\n",
    "        use_cache: bool = True,\n",
    "        dtype: Optional[Dict[str, object]] = None,\n",
    "        **pd_kw,\n",
    "    ) -> pd.DataFrame:\n",
    "        urls, checksum = self._urls_for(name)\n",
    "        cache_path = self._cache_path_for(name)\n",
    "\n",
    "        if use_cache and cache_path is not None and cache_path.exists():\n",
    "            return pd.read_csv(cache_path, compression=\"gzip\", dtype=dtype, **pd_kw)\n",
    "\n",
    "        last_err = None\n",
    "        for ext in [\".csv.gz\", \".csv\"]:\n",
    "            url = urls.get(ext)\n",
    "            if not url:\n",
    "                continue\n",
    "            try:\n",
    "                resp = requests.get(url, headers=self.headers, timeout=self.timeout, stream=False)\n",
    "            except requests.RequestException as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "\n",
    "            if resp.status_code == 200:\n",
    "                content = resp.content\n",
    "\n",
    "                # Verify checksum if coming from Zenodo and checksum is known\n",
    "                if self.source == \"zenodo\" and checksum is not None and ext in checksum:\n",
    "                    algo, expected = checksum[ext]\n",
    "                    if not self._verify_checksum(content, algo, expected):\n",
    "                        last_err = RuntimeError(f\"Checksum mismatch for {name}{ext}\")\n",
    "                        continue\n",
    "\n",
    "                # cache gz bytes if applicable\n",
    "                if use_cache and cache_path is not None and ext == \".csv.gz\":\n",
    "                    try:\n",
    "                        cache_path.write_bytes(content)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                buf = io.BytesIO(content)\n",
    "                if ext == \".csv.gz\":\n",
    "                    return pd.read_csv(buf, compression=\"gzip\", dtype=dtype, **pd_kw)\n",
    "                else:\n",
    "                    return pd.read_csv(buf, compression=None, dtype=dtype, **pd_kw)\n",
    "            else:\n",
    "                last_err = RuntimeError(f\"HTTP {resp.status_code} for {url}\")\n",
    "\n",
    "        avail = self.available_names(refresh=True)\n",
    "        suggestions = self.suggest(name) if avail else []\n",
    "        tried = [u for u in [urls.get(\".csv.gz\"), urls.get(\".csv\")] if u]\n",
    "        msg_lines = [\n",
    "            f\"Failed to fetch dataset '{name}' for task '{self.task}'.\",\n",
    "            f\"Source: {self.source}\",\n",
    "            \"Tried URLs (in order):\",\n",
    "        ] + [f\"  {u}\" for u in tried]\n",
    "\n",
    "        if avail:\n",
    "            msg_lines.append(\"\")\n",
    "            msg_lines.append(\"Available dataset names:\")\n",
    "            if len(avail) > 200:\n",
    "                msg_lines.append(f\"  (showing first 200 of {len(avail)}):\")\n",
    "                avail_display = avail[:200]\n",
    "            else:\n",
    "                avail_display = avail\n",
    "            msg_lines += [f\"  {n}\" for n in avail_display]\n",
    "            if suggestions:\n",
    "                msg_lines.append(\"\")\n",
    "                msg_lines.append(f\"Did you mean: {suggestions} ?\")\n",
    "\n",
    "        msg_lines.append(\"\")\n",
    "        msg_lines.append(f\"Last error: {last_err!s}\")\n",
    "        raise FileNotFoundError(\"\\n\".join(msg_lines))\n",
    "\n",
    "    def load_many(\n",
    "        self,\n",
    "        names: Iterable[str],\n",
    "        use_cache: bool = True,\n",
    "        dtype: Optional[Dict[str, object]] = None,\n",
    "        parallel: bool = True,\n",
    "        **pd_kw,\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        names_list = list(names)\n",
    "        results: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "        if not parallel or self.max_workers <= 1 or len(names_list) == 1:\n",
    "            for nm in names_list:\n",
    "                try:\n",
    "                    results[nm] = self.load(nm, use_cache=use_cache, dtype=dtype, **pd_kw)\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Failed to load {self.task}/{nm}: {e}\") from e\n",
    "            return results\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as ex:\n",
    "            futures = {\n",
    "                ex.submit(self.load, nm, use_cache, dtype, **pd_kw): nm\n",
    "                for nm in names_list\n",
    "            }\n",
    "            for fut in as_completed(futures):\n",
    "                nm = futures[fut]\n",
    "                try:\n",
    "                    results[nm] = fut.result()\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Failed to load {self.task}/{nm}: {e}\") from e\n",
    "        return results\n",
    "\n",
    "    # ------------- Internal: URL building -------------\n",
    "    def _urls_for(self, name: str) -> Tuple[Dict[str, str], Optional[Dict[str, Tuple[str, str]]]]:\n",
    "        if self.source == \"zenodo\":\n",
    "            urls, checksums = self._zenodo_urls_for(name)\n",
    "            # fallback to GitHub if not found in Zenodo (optional, keep behavior consistent)\n",
    "            if urls:\n",
    "                # ensure both keys exist for stable iteration order in load()\n",
    "                if \".csv.gz\" not in urls:\n",
    "                    urls[\".csv.gz\"] = f\"{self._gh_raw_base}/{self.task}/{name}.csv.gz\"\n",
    "                if \".csv\" not in urls:\n",
    "                    urls[\".csv\"] = f\"{self._gh_raw_base}/{self.task}/{name}.csv\"\n",
    "                return urls, checksums\n",
    "        # default: GitHub raw\n",
    "        base = f\"{self._gh_raw_base}/{self.task}/{name}\"\n",
    "        urls = {\".csv.gz\": f\"{base}.csv.gz\", \".csv\": f\"{base}.csv\"}\n",
    "        return urls, None\n",
    "\n",
    "    def _zenodo_urls_for(self, name: str) -> Tuple[Dict[str, str], Dict[str, Tuple[str, str]]]:\n",
    "        if not self.zenodo_record:\n",
    "            return {}, {}\n",
    "        index = self._zenodo_file_index or self._build_zenodo_index()\n",
    "        if not index:\n",
    "            return {}, {}\n",
    "\n",
    "        rel_gz = f\"Data/{self.task}/{name}.csv.gz\"\n",
    "        rel_csv = f\"Data/{self.task}/{name}.csv\"\n",
    "        urls: Dict[str, str] = {}\n",
    "        checksums: Dict[str, Tuple[str, str]] = {}\n",
    "\n",
    "        if rel_gz in index:\n",
    "            f = index[rel_gz]\n",
    "            urls[\".csv.gz\"] = f[\"links\"][\"download\"]\n",
    "            algo, hexdigest = self._parse_checksum(f.get(\"checksum\", \"\"))\n",
    "            if algo and hexdigest:\n",
    "                checksums[\".csv.gz\"] = (algo, hexdigest)\n",
    "\n",
    "        if rel_csv in index:\n",
    "            f = index[rel_csv]\n",
    "            urls[\".csv\"] = f[\"links\"][\"download\"]\n",
    "            algo, hexdigest = self._parse_checksum(f.get(\"checksum\", \"\"))\n",
    "            if algo and hexdigest:\n",
    "                checksums[\".csv\"] = (algo, hexdigest)\n",
    "\n",
    "        return urls, checksums\n",
    "\n",
    "    def _build_zenodo_index(self) -> Dict[str, Dict]:\n",
    "        api = _ZENODO_API_TPL.format(record_id=self.zenodo_record)\n",
    "        try:\n",
    "            r = requests.get(api, headers=self.headers, timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            meta = r.json()\n",
    "        except requests.RequestException:\n",
    "            self._zenodo_file_index = {}\n",
    "            return {}\n",
    "\n",
    "        files = meta.get(\"files\", [])\n",
    "        # Build index of {key -> file_meta}\n",
    "        idx = {f.get(\"key\", \"\"): f for f in files if f.get(\"key\")}\n",
    "        self._zenodo_file_index = idx\n",
    "        return idx\n",
    "\n",
    "    # ------------- Internal: names fetching -------------\n",
    "    def _fetch_names_github(self) -> List[str]:\n",
    "        try:\n",
    "            r = requests.get(self._gh_api_url, headers=self.headers, timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            items = r.json()\n",
    "        except requests.RequestException:\n",
    "            return []\n",
    "\n",
    "        names = set()\n",
    "        for it in items:\n",
    "            nm = it.get(\"name\", \"\")\n",
    "            if nm.endswith(\".csv.gz\"):\n",
    "                names.add(nm[:-len(\".csv.gz\")])\n",
    "            elif nm.endswith(\".csv\"):\n",
    "                names.add(nm[:-len(\".csv\")])\n",
    "        return sorted(names)\n",
    "\n",
    "    def _fetch_names_zenodo(self) -> List[str]:\n",
    "        index = self._zenodo_file_index or self._build_zenodo_index()\n",
    "        base = f\"Data/{self.task}/\"\n",
    "        names = set()\n",
    "        for key in index.keys():\n",
    "            if not key.startswith(base):\n",
    "                continue\n",
    "            if key.endswith(\".csv.gz\"):\n",
    "                names.add(key[len(base):-len(\".csv.gz\")])\n",
    "            elif key.endswith(\".csv\"):\n",
    "                names.add(key[len(base):-len(\".csv\")])\n",
    "        return sorted(names)\n",
    "\n",
    "    # ------------- Internal: cache, checksum, helpers -------------\n",
    "    def _cache_path_for(self, name: str) -> Optional[Path]:\n",
    "        if not self.cache_dir:\n",
    "            return None\n",
    "        return (self.cache_dir / f\"{self.task}__{name}.csv.gz\").resolve()\n",
    "\n",
    "    def _verify_checksum(self, data: bytes, algo: str, expected_hex: str) -> bool:\n",
    "        algo = algo.lower()\n",
    "        if algo in {\"md5\", \"sha1\", \"sha224\", \"sha256\", \"sha384\", \"sha512\"}:\n",
    "            h = hashlib.new(algo)\n",
    "            h.update(data)\n",
    "            return h.hexdigest().lower() == expected_hex.lower()\n",
    "        return False\n",
    "\n",
    "    def _parse_checksum(self, checksum_field: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "        # Zenodo format: \"md5:abcdef123...\" or \"sha256:abcdef...\"\n",
    "        if not checksum_field:\n",
    "            return None, None\n",
    "        m = re.match(r\"^(md5|sha1|sha224|sha256|sha384|sha512):([0-9A-Fa-f]+)$\", checksum_field.strip())\n",
    "        if not m:\n",
    "            return None, None\n",
    "        return m.group(1), m.group(2)\n",
    "\n",
    "    def _infer_zenodo_record(self, record: Optional[int], doi: Optional[str]) -> Optional[int]:\n",
    "        if record is not None:\n",
    "            return int(record)\n",
    "        if not doi:\n",
    "            return None\n",
    "        # Accept forms like \"10.5281/zenodo.17297723\" or \"https://doi.org/10.5281/zenodo.17297723\"\n",
    "        # Extract the trailing numeric ID\n",
    "        m = re.search(r\"zenodo\\.(\\d+)$\", doi)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        m = re.search(r\"/records/(\\d+)$\", doi)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        # Fallback: digits at the end\n",
    "        m = re.search(r\"(\\d+)$\", doi)\n",
    "        return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Usage examples\n",
    "# ---------------------------\n",
    "# 1) Latest (mutable) from GitHub main:\n",
    "# dl = DataLoader(task=\"aam\", source=\"github\", ref=\"main\", ref_type=\"heads\")\n",
    "# df = dl.load(\"ecoli\")\n",
    "\n",
    "# 2) Reproducible GitHub tag:\n",
    "# dl = DataLoader(task=\"aam\", source=\"github\", ref=\"v0.0.5\", ref_type=\"tags\")\n",
    "# df = dl.load(\"ecoli\")\n",
    "\n",
    "# 3) Archival Zenodo version (with checksum validation):\n",
    "# dl = DataLoader(task=\"aam\", source=\"zenodo\", zenodo_record=17297723)\n",
    "# df = dl.load(\"ecoli\")\n",
    "\n",
    "# 4) Zenodo by DOI (auto-parse record id):\n",
    "# dl = DataLoader(task=\"aam\", source=\"zenodo\", zenodo_doi=\"10.5281/zenodo.17297723\")\n",
    "# df = dl.load(\"ecoli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Archival Zenodo version (with checksum validation):\n",
    "dl = DataLoader(task=\"aam\", source=\"zenodo\", zenodo_record=17297723)\n",
    "df = dl.load(\"ecoli\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(task=\"aam\", source=\"zenodo\", zenodo_doi=\"10.5281/zenodo.17297723\")\n",
    "df = dl.load(\"ecoli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1844242779.py, line 166)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 166\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m*[f\"  {u}\" for u in tried] if tried else [\"  (none found at Zenodo/GitHub lookup paths)\"],\u001b[39m\n                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "import hashlib\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "_ZENODO_RECORD_API = \"https://zenodo.org/api/records/{record_id}\"\n",
    "_ZENODO_SEARCH_API = \"https://zenodo.org/api/records\"\n",
    "_GH_RAW_TPL = \"https://raw.githubusercontent.com/{owner}/{repo}/refs/{ref_type}/{ref}/Data\"\n",
    "_GH_API_TPL = \"https://api.github.com/repos/{owner}/{repo}/contents/Data/{task}?ref={ref}\"\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        task: str,\n",
    "        concept_doi: str = \"10.5281/zenodo.17297258\",\n",
    "        version: Optional[str] = None,\n",
    "        cache_dir: Optional[Path] = None,\n",
    "        timeout: int = 20,\n",
    "        user_agent: str = \"SynRXN-DataLoader/1.1\",\n",
    "        max_workers: int = 6,\n",
    "        gh_owner: str = \"TieuLongPhan\",\n",
    "        gh_repo: str = \"SynRXN\",\n",
    "        gh_ref: Optional[str] = None,\n",
    "        gh_enable: bool = True,\n",
    "    ) -> None:\n",
    "        self.task = str(task).strip(\"/\")\n",
    "        self.concept_doi = concept_doi.strip()\n",
    "        self.version = version.strip() if isinstance(version, str) else None\n",
    "        self.timeout = int(timeout)\n",
    "        self.headers = {\"User-Agent\": user_agent}\n",
    "        self.max_workers = int(max_workers)\n",
    "\n",
    "        # GitHub fallback config\n",
    "        self.gh_enable = bool(gh_enable)\n",
    "        self.gh_owner = gh_owner\n",
    "        self.gh_repo = gh_repo\n",
    "        # If user didnâ€™t provide a ref: derive from version (vX first, then X), else default main\n",
    "        self._gh_try_refs: List[Tuple[str, str]] = []  # list of (ref_type, ref)\n",
    "        if self.gh_enable:\n",
    "            if gh_ref:\n",
    "                self._gh_try_refs = [(\"heads\", gh_ref)]\n",
    "            elif self.version:\n",
    "                self._gh_try_refs = [(\"tags\", f\"v{self.version}\"), (\"tags\", self.version)]\n",
    "            else:\n",
    "                self._gh_try_refs = [(\"heads\", \"main\")]\n",
    "\n",
    "        self.cache_dir: Optional[Path] = Path(cache_dir).expanduser().resolve() if cache_dir else None\n",
    "        if self.cache_dir:\n",
    "            self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Resolved Zenodo record + file index\n",
    "        self._record_id: Optional[int] = self._resolve_record_id(self.concept_doi, self.version)\n",
    "        self._file_index: Dict[str, Dict] = self._build_file_index(self._record_id)\n",
    "\n",
    "        # Names caches\n",
    "        self._names_cache_zenodo: Optional[List[str]] = None\n",
    "        self._names_cache_github: Optional[List[str]] = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f\"DataLoader(task={self.task!r}, concept_doi={self.concept_doi!r}, \"\n",
    "                f\"version={self.version!r}, record_id={self._record_id}, \"\n",
    "                f\"github={self.gh_owner}/{self.gh_repo}, gh_refs={self._gh_try_refs}, \"\n",
    "                f\"cache_dir={str(self.cache_dir) if self.cache_dir else None})\")\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        ver = self.version or \"latest\"\n",
    "        return f\"<DataLoader zenodo:{self.concept_doi} [{ver}] + github:{self.gh_owner}/{self.gh_repo} task={self.task}>\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.names)\n",
    "\n",
    "    def __contains__(self, name: str) -> bool:\n",
    "        return name in self.names\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.names\n",
    "\n",
    "    @property\n",
    "    def names(self) -> List[str]:\n",
    "        return self.available_names()\n",
    "\n",
    "    def available_names(self, refresh: bool = False) -> List[str]:\n",
    "        z_names = self._available_names_zenodo(refresh=refresh)\n",
    "        g_names = self._available_names_github(refresh=refresh) if self.gh_enable else []\n",
    "        return sorted(set(z_names).union(g_names))\n",
    "\n",
    "    def refresh_names(self) -> List[str]:\n",
    "        self._file_index = self._build_file_index(self._record_id)\n",
    "        self._names_cache_zenodo = None\n",
    "        self._names_cache_github = None\n",
    "        return self.available_names(refresh=True)\n",
    "\n",
    "    def suggest(self, name: str, n: int = 5) -> List[str]:\n",
    "        import difflib\n",
    "        names = self.available_names()\n",
    "        if not names:\n",
    "            return []\n",
    "        return difflib.get_close_matches(name, names, n=n, cutoff=0.4)\n",
    "\n",
    "    def print_names(self, cols: int = 3, show_count: bool = True) -> None:\n",
    "        names = self.available_names()\n",
    "        if show_count:\n",
    "            print(f\"Datasets in task '{self.task}': {len(names)}\")\n",
    "        if not names:\n",
    "            print(\"  (no names found)\")\n",
    "            return\n",
    "        rows = math.ceil(len(names) / cols)\n",
    "        padded = names + [\"\"] * (rows * cols - len(names))\n",
    "        matrix = [padded[i: i + rows] for i in range(0, rows * cols, rows)]\n",
    "        for r in range(rows):\n",
    "            row_items = [matrix[c][r].ljust(30) for c in range(cols) if matrix[c][r]]\n",
    "            print(\"  \" + \"  \".join(row_items))\n",
    "\n",
    "    def load(self, name: str, use_cache: bool = True, dtype: Optional[Dict[str, object]] = None, **pd_kw) -> pd.DataFrame:\n",
    "        urls, checksums = self._urls_for(name)\n",
    "        cache_path = self._cache_path_for(name)\n",
    "\n",
    "        if use_cache and cache_path is not None and cache_path.exists():\n",
    "            return pd.read_csv(cache_path, compression=\"gzip\", dtype=dtype, **pd_kw)\n",
    "\n",
    "        last_err = None\n",
    "        tried = []\n",
    "        for ext in [\".csv.gz\", \".csv\"]:\n",
    "            url = urls.get(ext)\n",
    "            if not url:\n",
    "                continue\n",
    "            tried.append(url)\n",
    "            try:\n",
    "                resp = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "            except requests.RequestException as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "            if resp.status_code == 200:\n",
    "                content = resp.content\n",
    "                if ext in checksums:\n",
    "                    algo, expected = checksums[ext]\n",
    "                    if not self._verify_checksum(content, algo, expected):\n",
    "                        last_err = RuntimeError(f\"Checksum mismatch for {name}{ext}\")\n",
    "                        continue\n",
    "                if use_cache and cache_path is not None and ext == \".csv.gz\":\n",
    "                    try:\n",
    "                        cache_path.write_bytes(content)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                buf = io.BytesIO(content)\n",
    "                if ext == \".csv.gz\":\n",
    "                    return pd.read_csv(buf, compression=\"gzip\", dtype=dtype, **pd_kw)\n",
    "                else:\n",
    "                    return pd.read_csv(buf, compression=None, dtype=dtype, **pd_kw)\n",
    "            else:\n",
    "                last_err = RuntimeError(f\"HTTP {resp.status_code} for {url}\")\n",
    "\n",
    "        avail = self.available_names(refresh=True)\n",
    "        suggestions = self.suggest(name) if avail else []\n",
    "        msg = [\n",
    "            f\"Failed to fetch dataset '{name}' for task '{self.task}'.\",\n",
    "            f\"Concept DOI: {self.concept_doi}\",\n",
    "            f\"Version: {self.version or 'latest'} (record {self._record_id})\",\n",
    "            \"Tried URLs:\",\n",
    "            *[f\"  {u}\" for u in tried] if tried else [\"  (none found at Zenodo/GitHub lookup paths)\"],\n",
    "        ]\n",
    "        if avail:\n",
    "            msg.append(\"\")\n",
    "            msg.append(\"Available dataset names:\")\n",
    "            if len(avail) > 200:\n",
    "                msg.append(f\"  (showing first 200 of {len(avail)}):\")\n",
    "                avail_display = avail[:200]\n",
    "            else:\n",
    "                avail_display = avail\n",
    "            msg += [f\"  {n}\" for n in avail_display]\n",
    "            if suggestions:\n",
    "                msg.append(\"\")\n",
    "                msg.append(f\"Did you mean: {suggestions} ?\")\n",
    "        if last_err:\n",
    "            msg.append(\"\")\n",
    "            msg.append(f\"Last error: {last_err!s}\")\n",
    "        raise FileNotFoundError(\"\\n\".join(msg))\n",
    "\n",
    "    def load_many(self, names: Iterable[str], use_cache: bool = True, dtype: Optional[Dict[str, object]] = None, parallel: bool = True, **pd_kw) -> Dict[str, pd.DataFrame]:\n",
    "        names_list = list(names)\n",
    "        results: Dict[str, pd.DataFrame] = {}\n",
    "        if not parallel or self.max_workers <= 1 or len(names_list) == 1:\n",
    "            for nm in names_list:\n",
    "                try:\n",
    "                    results[nm] = self.load(nm, use_cache=use_cache, dtype=dtype, **pd_kw)\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Failed to load {self.task}/{nm}: {e}\") from e\n",
    "            return results\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as ex:\n",
    "            futures = {ex.submit(self.load, nm, use_cache, dtype, **pd_kw): nm for nm in names_list}\n",
    "            for fut in as_completed(futures):\n",
    "                nm = futures[fut]\n",
    "                try:\n",
    "                    results[nm] = fut.result()\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Failed to load {self.task}/{nm}: {e}\") from e\n",
    "        return results\n",
    "\n",
    "    # ---------- URL builders ----------\n",
    "    def _urls_for(self, name: str) -> Tuple[Dict[str, str], Dict[str, Tuple[str, str]]]:\n",
    "        urls: Dict[str, str] = {}\n",
    "        checksums: Dict[str, Tuple[str, str]] = {}\n",
    "\n",
    "        # Zenodo first\n",
    "        rel_gz = f\"Data/{self.task}/{name}.csv.gz\"\n",
    "        rel_csv = f\"Data/{self.task}/{name}.csv\"\n",
    "        if rel_gz in self._file_index:\n",
    "            f = self._file_index[rel_gz]\n",
    "            urls[\".csv.gz\"] = f[\"links\"][\"download\"]\n",
    "            algo, hex_ = self._parse_checksum(f.get(\"checksum\", \"\"))\n",
    "            if algo and hex_:\n",
    "                checksums[\".csv.gz\"] = (algo, hex_)\n",
    "        if rel_csv in self._file_index and \".csv.gz\" not in urls:\n",
    "            f = self._file_index[rel_csv]\n",
    "            urls[\".csv\"] = f[\"links\"][\"download\"]\n",
    "            algo, hex_ = self._parse_checksum(f.get(\"checksum\", \"\"))\n",
    "            if algo and hex_:\n",
    "                checksums[\".csv\"] = (algo, hex_)\n",
    "\n",
    "        # GitHub fallback (if missing)\n",
    "        if self.gh_enable and (\".csv.gz\" not in urls or \".csv\" not in urls):\n",
    "            for ref_type, ref in self._gh_try_refs:\n",
    "                base = _GH_RAW_TPL.format(owner=self.gh_owner, repo=self.gh_repo, ref_type=ref_type, ref=ref)\n",
    "                if \".csv.gz\" not in urls:\n",
    "                    urls[\".csv.gz\"] = f\"{base}/{self.task}/{name}.csv.gz\"\n",
    "                if \".csv\" not in urls:\n",
    "                    urls[\".csv\"] = f\"{base}/{self.task}/{name}.csv\"\n",
    "                # Only one ref set; if network fails it will move to next in load()\n",
    "                break\n",
    "\n",
    "        return urls, checksums\n",
    "\n",
    "    # ---------- Names ----------\n",
    "    def _available_names_zenodo(self, refresh: bool = False) -> List[str]:\n",
    "        if self._names_cache_zenodo is not None and not refresh:\n",
    "            return list(self._names_cache_zenodo)\n",
    "        base = f\"Data/{self.task}/\"\n",
    "        names = set()\n",
    "        for key in self._file_index.keys():\n",
    "            if not key.startswith(base):\n",
    "                continue\n",
    "            if key.endswith(\".csv.gz\"):\n",
    "                names.add(key[len(base):-len(\".csv.gz\")])\n",
    "            elif key.endswith(\".csv\"):\n",
    "                names.add(key[len(base):-len(\".csv\")])\n",
    "        self._names_cache_zenodo = sorted(names)\n",
    "        return list(self._names_cache_zenodo)\n",
    "\n",
    "    def _available_names_github(self, refresh: bool = False) -> List[str]:\n",
    "        if self._names_cache_github is not None and not refresh:\n",
    "            return list(self._names_cache_github)\n",
    "        names = set()\n",
    "        for ref_type, ref in self._gh_try_refs:\n",
    "            api_url = _GH_API_TPL.format(owner=self.gh_owner, repo=self.gh_repo, task=self.task, ref=ref)\n",
    "            try:\n",
    "                r = requests.get(api_url, headers=self.headers, timeout=self.timeout)\n",
    "                r.raise_for_status()\n",
    "                items = r.json()\n",
    "            except requests.RequestException:\n",
    "                continue\n",
    "            for it in items:\n",
    "                nm = it.get(\"name\", \"\")\n",
    "                if nm.endswith(\".csv.gz\"):\n",
    "                    names.add(nm[:-len(\".csv.gz\")])\n",
    "                elif nm.endswith(\".csv\"):\n",
    "                    names.add(nm[:-len(\".csv\")])\n",
    "            break  # only first successful ref\n",
    "        self._names_cache_github = sorted(names)\n",
    "        return list(self._names_cache_github)\n",
    "\n",
    "    # ---------- Zenodo resolution ----------\n",
    "    def _resolve_record_id(self, concept_doi: str, version: Optional[str]) -> int:\n",
    "        params = {\"q\": f'conceptdoi:\"{concept_doi}\"', \"all_versions\": 1, \"size\": 200}\n",
    "        r = requests.get(_ZENODO_SEARCH_API, params=params, headers=self.headers, timeout=self.timeout)\n",
    "        r.raise_for_status()\n",
    "        hits = r.json().get(\"hits\", {}).get(\"hits\", [])\n",
    "        if not hits:\n",
    "            raise RuntimeError(f\"No Zenodo records found for concept DOI {concept_doi}\")\n",
    "        if version:\n",
    "            target = self._normalize_version(version)\n",
    "            for h in hits:\n",
    "                meta_ver = self._normalize_version(h.get(\"metadata\", {}).get(\"version\", \"\"))\n",
    "                if meta_ver == target:\n",
    "                    return int(h[\"id\"])\n",
    "            # fallback to raw compare\n",
    "            for h in hits:\n",
    "                raw = str(h.get(\"metadata\", {}).get(\"version\", \"\")).strip()\n",
    "                if raw == version or raw == f\"v{version}\" or f\"v{raw}\" == version:\n",
    "                    return int(h[\"id\"])\n",
    "            raise RuntimeError(f\"Version '{version}' not found under {concept_doi}. \"\n",
    "                               f\"Available: {sorted({h.get('metadata', {}).get('version','') for h in hits})}\")\n",
    "        hits_sorted = sorted(hits, key=lambda h: h.get(\"updated\", h.get(\"created\", \"\")), reverse=True)\n",
    "        return int(hits_sorted[0][\"id\"])\n",
    "\n",
    "    def _build_file_index(self, record_id: Optional[int]) -> Dict[str, Dict]:\n",
    "        if record_id is None:\n",
    "            return {}\n",
    "        url = _ZENODO_RECORD_API.format(record_id=record_id)\n",
    "        r = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "        r.raise_for_status()\n",
    "        meta = r.json()\n",
    "        files = meta.get(\"files\", [])\n",
    "        return {f.get(\"key\", \"\"): f for f in files if f.get(\"key\")}\n",
    "\n",
    "    # ---------- Utils ----------\n",
    "    def _normalize_version(self, v: str) -> str:\n",
    "        v = str(v).strip()\n",
    "        if v.lower().startswith(\"v\"):\n",
    "            v = v[1:]\n",
    "        return v\n",
    "\n",
    "    def _parse_checksum(self, checksum_field: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "        if not checksum_field:\n",
    "            return None, None\n",
    "        m = re.match(r\"^(md5|sha1|sha224|sha256|sha384|sha512):([0-9A-Fa-f]+)$\", checksum_field.strip())\n",
    "        if not m:\n",
    "            return None, None\n",
    "        return m.group(1), m.group(2)\n",
    "\n",
    "    def _verify_checksum(self, data: bytes, algo: str, expected_hex: str) -> bool:\n",
    "        algo = algo.lower()\n",
    "        if algo in {\"md5\", \"sha1\", \"sha224\", \"sha256\", \"sha384\", \"sha512\"}:\n",
    "            h = hashlib.new(algo)\n",
    "            h.update(data)\n",
    "            return h.hexdigest().lower() == expected_hex.lower()\n",
    "        return False\n",
    "\n",
    "    def _cache_path_for(self, name: str) -> Optional[Path]:\n",
    "        if not self.cache_dir:\n",
    "            return None\n",
    "        return (self.cache_dir / f\"{self.task}__{name}.csv.gz\").resolve()\n",
    "\n",
    "# -------------------------\n",
    "# Usage\n",
    "# -------------------------\n",
    "# 1) Pin to specific Zenodo version, fallback to GitHub tag v{version}:\n",
    "# dl = DataLoader(task=\"aam\", concept_doi=\"10.5281/zenodo.17297258\", version=\"0.0.5\")\n",
    "# df = dl.load(\"ecoli\")\n",
    "\n",
    "# 2) Latest Zenodo, fallback to GitHub main:\n",
    "# dl = DataLoader(task=\"aam\", concept_doi=\"10.5281/zenodo.17297258\", version=None)\n",
    "# df = dl.load(\"some_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Failed to fetch dataset 'ecoli' for task 'aam'.\nConcept DOI: 10.5281/zenodo.17297258\nVersion: 0.0.5 (record 17297723)\nTried URLs:\n\nLast error: None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2) Pin to a specific version label (e.g., \"0.0.5\" or \"v0.0.5\"):\u001b[39;00m\n\u001b[32m      2\u001b[39m dl = DataLoader(task=\u001b[33m\"\u001b[39m\u001b[33maam\u001b[39m\u001b[33m\"\u001b[39m, concept_doi=\u001b[33m\"\u001b[39m\u001b[33m10.5281/zenodo.17297258\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m0.0.5\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mdl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mecoli\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 213\u001b[39m, in \u001b[36mDataLoader.load\u001b[39m\u001b[34m(self, name, use_cache, dtype, **pd_kw)\u001b[39m\n\u001b[32m    211\u001b[39m msg.append(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    212\u001b[39m msg.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLast error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_err\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(msg))\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Failed to fetch dataset 'ecoli' for task 'aam'.\nConcept DOI: 10.5281/zenodo.17297258\nVersion: 0.0.5 (record 17297723)\nTried URLs:\n\nLast error: None"
     ]
    }
   ],
   "source": [
    "# 2) Pin to a specific version label (e.g., \"0.0.5\" or \"v0.0.5\"):\n",
    "dl = DataLoader(task=\"aam\", concept_doi=\"10.5281/zenodo.17297258\", version=\"0.0.5\")\n",
    "df = dl.load(\"ecoli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnxdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
