{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. USPTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from synrxn.io.io import  save_df_gz\n",
    "from synkit.IO import configure_warnings_and_logs\n",
    "\n",
    "from synrbl import ReactionRebalancer, RebalanceConfig # install: pip install synrbl\n",
    "import pandas as pd\n",
    "configure_warnings_and_logs(True, True)\n",
    "url = \"https://raw.githubusercontent.com/TieuLongPhan/SynRBL/refs/heads/main/Data/Raw_data/USPTO/USPTO_50K.csv\"\n",
    "df = pd.read_csv(url).to_dict('records')\n",
    "for key, value in enumerate(df):\n",
    "    value['R-id'] = f'R_{key}'\n",
    "\n",
    "config = RebalanceConfig(reaction_col=\"reactions\", id_col=\"R-id\", n_jobs=2, batch_size=500,\n",
    "                        enable_logging=False, use_default_reduction=True)\n",
    "\n",
    "rebalancer = ReactionRebalancer(config=config, user_logger=None)\n",
    "result = rebalancer.rebalance(df, keep_extra=True)\n",
    "rbl = pd.DataFrame(result)\n",
    "rbl = rbl.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from rdkit import Chem\n",
    "\n",
    "def _split_reaction(reaction: str) -> Tuple[List[str], List[str]]:\n",
    "    if \">>\" in reaction:\n",
    "        left, right = reaction.split(\">>\", 1)\n",
    "    elif \">\" in reaction:\n",
    "        left, right = reaction.split(\">\", 1)\n",
    "    else:\n",
    "        raise ValueError(\"Reaction string must contain '>' or '>>' as separator.\")\n",
    "    left_mols = [s.strip() for s in left.split('.') if s.strip()]\n",
    "    right_mols = [s.strip() for s in right.split('.') if s.strip()]\n",
    "    return left_mols, right_mols\n",
    "\n",
    "def _mol_element_counts(smiles: str) -> Dict[str, int]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"Cannot parse SMILES: {smiles!r}\")\n",
    "    mol_h = Chem.AddHs(mol)\n",
    "    counts = defaultdict(int)\n",
    "    for a in mol_h.GetAtoms():\n",
    "        counts[a.GetSymbol()] += 1\n",
    "    return dict(counts)\n",
    "\n",
    "def _sum_counts(smiles_list: List[str]) -> Dict[str, int]:\n",
    "    total = defaultdict(int)\n",
    "    for s in smiles_list:\n",
    "        c = _mol_element_counts(s)\n",
    "        for el, n in c.items():\n",
    "            total[el] += n\n",
    "    return dict(total)\n",
    "\n",
    "def reaction_missing_side(\n",
    "    reaction: str,\n",
    "    *,\n",
    "    return_details: bool = False\n",
    ") -> Union[str, Dict]:\n",
    "    \"\"\"\n",
    "    Analyze reaction SMILES and return one of:\n",
    "      - \"one-side\"  : all non-zero element differences have the same sign\n",
    "      - \"both\"      : some elements are more on left and some more on right\n",
    "      - \"balanced\"  : no element differences\n",
    "\n",
    "    If return_details=True, returns dict:\n",
    "      {\"status\": <label>, \"diff\": {element: left-right, ...}}\n",
    "\n",
    "    Example:\n",
    "      reaction_missing_side(\"A>>B\") -> \"one-side\" / \"both\" / \"balanced\"\n",
    "    \"\"\"\n",
    "    left_mols, right_mols = _split_reaction(reaction)\n",
    "    left_counts = _sum_counts(left_mols)\n",
    "    right_counts = _sum_counts(right_mols)\n",
    "\n",
    "    elems = set(left_counts) | set(right_counts)\n",
    "    diff = {el: left_counts.get(el, 0) - right_counts.get(el, 0) for el in elems}\n",
    "\n",
    "    pos = any(v > 0 for v in diff.values())\n",
    "    neg = any(v < 0 for v in diff.values())\n",
    "\n",
    "    if not pos and not neg:\n",
    "        status = \"balanced\"\n",
    "    elif pos and neg:\n",
    "        status = \"both\"\n",
    "    else:\n",
    "        status = \"one-side\"\n",
    "\n",
    "    if return_details:\n",
    "        return {\"status\": status, \"diff\": diff}\n",
    "    return status\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "def _get_first_present(d: Dict[str, Any], keys: List[str]) -> Optional[Any]:\n",
    "    for k in keys:\n",
    "        if k in d and d[k] not in (\"\", None):\n",
    "            v = d[k]\n",
    "            # if it's a string, strip whitespace\n",
    "            return v.strip() if isinstance(v, str) else v\n",
    "    return None\n",
    "\n",
    "def curate_records(records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Curate a list of reaction-record dicts.\n",
    "\n",
    "    For each record produce a new dict with exactly:\n",
    "      - \"R-id\"         : from 'R-id' (fallbacks: 'R_id', 'rid', 'id')\n",
    "      - \"rxn\"          : from 'input_reaction' (fallbacks: 'reactions', 'rxn')\n",
    "      - \"ground_truth\" : from 'standardized_reactions' (fallbacks: 'standardized_reaction',\n",
    "                         'new_products', 'ground_truth')\n",
    "\n",
    "    Records missing any of these will still be included but the missing values will be None.\n",
    "    Records missing any reasonable identifier will be skipped.\n",
    "    \"\"\"\n",
    "    curated: List[Dict[str, Any]] = []\n",
    "    for rec in records:\n",
    "        # find R-id (several common alternatives)\n",
    "        rid = _get_first_present(rec, [\"R-id\", \"R_id\", \"rid\", \"id\"])\n",
    "        if rid is None:\n",
    "            # skip records with no identifier\n",
    "            continue\n",
    "\n",
    "        rxn = _get_first_present(rec, [\"input_reaction\", \"reactions\", \"rxn\"])\n",
    "        ground_truth = _get_first_present(\n",
    "            rec,\n",
    "            [\n",
    "                \"standardized_reactions\",\n",
    "                \"standardized_reaction\",\n",
    "                \"new_products\",\n",
    "                \"ground_truth\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        curated.append({\"R-id\": rid, \"rxn\": rxn, \"ground_truth\": ground_truth})\n",
    "    return pd.DataFrame(curated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon = []\n",
    "mnc = []\n",
    "mos = []\n",
    "mbs = []\n",
    "for value in rbl:\n",
    "    if value['success']:\n",
    "        if value['solved_by'] == 'mcs-based':\n",
    "            if value['confidence'] > 0.9:\n",
    "                carbon.append(value)\n",
    "        elif value['solved_by'] == 'rule-based':\n",
    "            mnc.append(value)\n",
    "\n",
    "for value in carbon:\n",
    "    if reaction_missing_side(value['input_reaction']) == 'one-side':\n",
    "        mos.append(value)\n",
    "    else:\n",
    "        mbs.append(value)\n",
    "\n",
    "mos = curate_records(mos)\n",
    "mbs = curate_records(mbs)\n",
    "mnc = curate_records(mnc)\n",
    "save_df_gz(mos, '../Data/rbl/mos.csv.gz')\n",
    "save_df_gz(mbs, '../Data/rbl/mbs.csv.gz')\n",
    "save_df_gz(mnc, '../Data/rbl/mnc.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mos))\n",
    "print(len(mbs))\n",
    "print(len(mnc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Complex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/TieuLongPhan/SynRBL/refs/heads/main/Data/Validation_set/validation_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any, Iterable\n",
    "import pandas as pd\n",
    "import logging\n",
    "import ast\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _normalize_to_list(value: Any) -> List[str]:\n",
    "    \"\"\"\n",
    "    Normalize `datasets`-like or list-like values into a Python list of strings.\n",
    "    Accepts:\n",
    "      - actual list/tuple of str -> returns list\n",
    "      - string that is a Python literal list (\"['a','b']\") -> ast.literal_eval -> list\n",
    "      - simple string -> [string]\n",
    "    Any non-string objects inside lists/tuples are converted to str.\n",
    "    Empty or None -> []\n",
    "    \"\"\"\n",
    "    if value is None or (isinstance(value, float) and pd.isna(value)):\n",
    "        return []\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        return [str(x) for x in value if x is not None and str(x).strip() != \"\"]\n",
    "    if isinstance(value, str):\n",
    "        s = value.strip()\n",
    "        # try to parse python literal list/tuple e.g. \"['a','b']\"\n",
    "        if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, (list, tuple)):\n",
    "                    return [str(x) for x in parsed if x is not None and str(x).strip() != \"\"]\n",
    "            except Exception:\n",
    "                # fall through to treat as plain string\n",
    "                pass\n",
    "        if s == \"\":\n",
    "            return []\n",
    "        return [s]\n",
    "    # fallback: convert to single string\n",
    "    return [str(value)]\n",
    "\n",
    "\n",
    "def _extract_first_str(value: Any) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given a possibly list-like or string representation of a reaction, return the first\n",
    "    non-empty string element, or None.\n",
    "    \"\"\"\n",
    "    if value is None or (isinstance(value, float) and pd.isna(value)):\n",
    "        return None\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        for el in value:\n",
    "            if el is None:\n",
    "                continue\n",
    "            s = str(el).strip()\n",
    "            if s:\n",
    "                return s\n",
    "        return None\n",
    "    if isinstance(value, str):\n",
    "        s = value.strip()\n",
    "        # attempt to parse stringified list (e.g. \"['a','b']\") and pick first element\n",
    "        if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, (list, tuple)) and parsed:\n",
    "                    return str(parsed[0]).strip() or None\n",
    "            except Exception:\n",
    "                # not a literal list -> treat as raw string\n",
    "                pass\n",
    "        return s or None\n",
    "    # fallback: coerce to str\n",
    "    s = str(value).strip()\n",
    "    return s if s else None\n",
    "\n",
    "\n",
    "def clean_synrbl(\n",
    "    data: pd.DataFrame,\n",
    "    std: Optional[Any] = None,\n",
    "    drop_cols: Optional[List[str]] = None,\n",
    "    dataset_include: Optional[Iterable[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and curate a SynRBL dataframe.\n",
    "\n",
    "    Enhancements over the original:\n",
    "      - Filters rows by presence of any substring in `dataset_include` inside the `datasets` column.\n",
    "        Default dataset_include = ['golden_dataset', 'Jaworski'].\n",
    "      - Robust parsing for `datasets` that may be lists, tuples, or string representations.\n",
    "      - Safely extracts the first element from list-like `expected_reaction` / `reaction` fields.\n",
    "      - Adds logging and better error capture.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        Input DataFrame. Must contain `expected_reaction` column and ideally a `datasets` column.\n",
    "    std\n",
    "        Standardizer instance exposing `.fit(x)` used to normalize reactions.\n",
    "        If None, will instantiate `synkit.Chem.Reaction.standardize.Standardize()`.\n",
    "    drop_cols\n",
    "        Columns to drop before processing. Defaults to `['R-ids', 'wrong_reactions']`.\n",
    "    dataset_include\n",
    "        Iterable of substrings to match against entries in `datasets` column (case-insensitive).\n",
    "        Rows are kept if any element of `datasets` contains any of these substrings.\n",
    "        Default: ['golden_dataset', 'Jaworski'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns ['R-id','rxn','ground_truth','error'], filtered and standardized.\n",
    "    \"\"\"\n",
    "    if std is None:\n",
    "        # lazy import so function can be imported without synkit installed\n",
    "        from synkit.Chem.Reaction.standardize import Standardize\n",
    "\n",
    "        std = Standardize()\n",
    "\n",
    "    if drop_cols is None:\n",
    "        drop_cols = [\"R-ids\", \"wrong_reactions\"]\n",
    "\n",
    "    if dataset_include is None:\n",
    "        dataset_include = [\"golden_dataset\", \"Jaworski\"]\n",
    "\n",
    "    # normalize dataset_include to lower-case strings for substring matching\n",
    "    dataset_include_lc = [str(x).lower() for x in dataset_include]\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    # drop configured columns if present\n",
    "    cols_to_drop = [c for c in drop_cols if c in df.columns]\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # require expected_reaction present\n",
    "    if \"expected_reaction\" not in df.columns:\n",
    "        raise ValueError(\"Input dataframe must contain an 'expected_reaction' column.\")\n",
    "    # drop rows with missing expected_reaction\n",
    "    df = df.dropna(subset=[\"expected_reaction\"])\n",
    "\n",
    "    # if datasets column exists, filter rows to keep only matching datasets\n",
    "    if \"datasets\" in df.columns:\n",
    "        keep_mask = []\n",
    "        for idx, row in df.iterrows():\n",
    "            ds_raw = row[\"datasets\"]\n",
    "            ds_list = _normalize_to_list(ds_raw)\n",
    "            # check any dataset element contains any include substring (case-insensitive)\n",
    "            matched = False\n",
    "            for ds in ds_list:\n",
    "                ds_lc = ds.lower()\n",
    "                if any(key in ds_lc for key in dataset_include_lc):\n",
    "                    matched = True\n",
    "                    break\n",
    "            keep_mask.append(matched)\n",
    "        # apply mask and log counts\n",
    "        keep_series = pd.Series(keep_mask, index=df.index)\n",
    "        kept = keep_series.sum()\n",
    "        logger.info(\"Dataset filter: keeping %d / %d rows matching %s\", kept, len(df), list(dataset_include))\n",
    "        df = df[keep_series]\n",
    "    else:\n",
    "        # if no datasets column, log and proceed without filtering\n",
    "        logger.info(\"No 'datasets' column found â€” skipping dataset filtering.\")\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        rec: Dict[str, Any] = {}\n",
    "        # preferred identifier: 'id' else index\n",
    "        rec[\"R-id\"] = row.get(\"id\", idx)\n",
    "\n",
    "        # ground truth extraction & standardization\n",
    "        gt_raw = _extract_first_str(row.get(\"expected_reaction\"))\n",
    "        gt_err: Optional[str] = None\n",
    "        try:\n",
    "            rec[\"ground_truth\"] = std.fit(gt_raw) if gt_raw is not None else None\n",
    "        except Exception as exc:\n",
    "            logger.exception(\"Standardization failed for ground_truth at R-id=%s\", rec[\"R-id\"])\n",
    "            rec[\"ground_truth\"] = None\n",
    "            gt_err = str(exc)\n",
    "\n",
    "        # reaction extraction & standardization (may be missing)\n",
    "        rxn_raw = _extract_first_str(row.get(\"reaction\")) if \"reaction\" in df.columns else None\n",
    "        rxn_err: Optional[str] = None\n",
    "        if rxn_raw is None:\n",
    "            rec[\"rxn\"] = None\n",
    "        else:\n",
    "            try:\n",
    "                rec[\"rxn\"] = std.fit(rxn_raw)\n",
    "            except Exception as exc:\n",
    "                logger.exception(\"Standardization failed for reaction at R-id=%s\", rec[\"R-id\"])\n",
    "                rec[\"rxn\"] = None\n",
    "                rxn_err = str(exc)\n",
    "\n",
    "        # attach error dict only when there were errors\n",
    "        errors: Dict[str, str] = {}\n",
    "        if gt_err:\n",
    "            errors[\"ground_truth\"] = gt_err\n",
    "        if rxn_err:\n",
    "            errors[\"reaction\"] = rxn_err\n",
    "        rec[\"error\"] = errors if errors else None\n",
    "\n",
    "        records.append(rec)\n",
    "\n",
    "    result_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    # ordered columns\n",
    "    cols_order = [\"R-id\", \"rxn\", \"ground_truth\", \"error\"]\n",
    "    for c in cols_order:\n",
    "        if c not in result_df.columns:\n",
    "            result_df[c] = None\n",
    "    result_df = result_df[cols_order]\n",
    "\n",
    "    # reset index for cleanliness\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex = clean_synrbl(df, dataset_include=[\"golden_dataset\", 'Jaworski'])\n",
    "complex.drop(columns={'error'}, inplace=True)\n",
    "save_df_gz(complex, '../Data/rbl/complex.csv.gz')\n",
    "print(len(complex))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_syntemp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
