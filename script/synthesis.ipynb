{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. USPTO 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from typing import Optional, Dict, Iterable\n",
    "import pandas as pd\n",
    "\n",
    "DEFAULT_PATTERNS = {\n",
    "    \"raw_train\": \"train\",\n",
    "    \"raw_val\": \"valid\",\n",
    "    \"raw_valid\": \"valid\",\n",
    "    \"raw_test\": \"test\",\n",
    "}\n",
    "\n",
    "def _normalize_dropbox_url(url: str) -> str:\n",
    "    if \"dropbox.com\" not in url:\n",
    "        return url\n",
    "    if \"dl=0\" in url:\n",
    "        return url.replace(\"dl=0\", \"dl=1\")\n",
    "    if \"dl=1\" in url:\n",
    "        return url\n",
    "    return url + (\"&dl=1\" if \"?\" in url else \"?dl=1\")\n",
    "\n",
    "def _find_member_for_pattern(namelist: Iterable[str], token: str) -> Optional[str]:\n",
    "    token_l = token.lower()\n",
    "    for nm in namelist:\n",
    "        if nm.lower().endswith(token_l) or nm.lower().endswith(token_l + \".csv\") or nm.lower().endswith(token_l + \".txt\"):\n",
    "            return nm\n",
    "    for nm in namelist:\n",
    "        if token_l in nm.lower():\n",
    "            return nm\n",
    "    return None\n",
    "\n",
    "def download_and_combine_raw_splits(\n",
    "    url: str,\n",
    "    patterns: Optional[Dict[str, str]] = None,\n",
    "    *,\n",
    "    encoding: str = \"utf-8\",\n",
    "    treat_txt_as_lines: bool = True,\n",
    "    save_csv: Optional[str] = None,\n",
    "    timeout: int = 60,\n",
    ") -> pd.DataFrame:\n",
    "    if patterns is None:\n",
    "        patterns = DEFAULT_PATTERNS\n",
    "\n",
    "    url = _normalize_dropbox_url(url)\n",
    "    resp = requests.get(url, stream=True, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    raw = resp.content\n",
    "    bio = io.BytesIO(raw)\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(bio) as zf:\n",
    "            namelist = zf.namelist()\n",
    "            dfs = []\n",
    "            for token, split_label in patterns.items():\n",
    "                member = _find_member_for_pattern(namelist, token)\n",
    "                if not member:\n",
    "                    continue\n",
    "                with zf.open(member) as fh:\n",
    "                    if member.lower().endswith(\".csv\"):\n",
    "                        df = pd.read_csv(io.TextIOWrapper(fh, encoding=encoding))\n",
    "                    else:\n",
    "                        text = fh.read().decode(encoding, errors=\"replace\")\n",
    "                        if treat_txt_as_lines:\n",
    "                            lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "                            df = pd.DataFrame({\"rxn\": lines})\n",
    "                        else:\n",
    "                            fh.seek(0)\n",
    "                            df = pd.read_csv(io.TextIOWrapper(fh, encoding=encoding))\n",
    "                    df[\"split\"] = split_label\n",
    "                    dfs.append(df)\n",
    "            if not dfs:\n",
    "                raise RuntimeError(\"No files matching raw_train/raw_val/raw_test found inside ZIP.\")\n",
    "            combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "            if save_csv:\n",
    "                combined.to_csv(save_csv, index=False)\n",
    "            return combined\n",
    "    except zipfile.BadZipFile:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        bio.seek(0)\n",
    "        df_all = pd.read_csv(io.BytesIO(raw))\n",
    "        if \"split\" in df_all.columns:\n",
    "            if save_csv:\n",
    "                df_all.to_csv(save_csv, index=False)\n",
    "            return df_all\n",
    "        inferred_split = None\n",
    "        lower_url = url.lower()\n",
    "        for token, split_label in patterns.items():\n",
    "            if token in lower_url:\n",
    "                inferred_split = split_label\n",
    "                break\n",
    "        if inferred_split is None:\n",
    "            inferred_split = \"train\"\n",
    "        df_all[\"split\"] = inferred_split\n",
    "        if save_csv:\n",
    "            df_all.to_csv(save_csv, index=False)\n",
    "        return df_all\n",
    "    except Exception:\n",
    "        text = raw.decode(encoding, errors=\"replace\")\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        lower_url = url.lower()\n",
    "        inferred_split = None\n",
    "        for token, split_label in patterns.items():\n",
    "            if token in lower_url:\n",
    "                inferred_split = split_label\n",
    "                break\n",
    "        if inferred_split is None:\n",
    "            inferred_split = \"train\"\n",
    "        df = pd.DataFrame({\"rxn\": lines})\n",
    "        df[\"split\"] = inferred_split\n",
    "        if save_csv:\n",
    "            df.to_csv(save_csv, index=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional, Union\n",
    "\n",
    "def curate_uspto_minimal(\n",
    "    df_or_path: Union[pd.DataFrame, str],\n",
    "    id_col: str = \"id\",\n",
    "    rxn_col: str = \"reactants>reagents>production\",\n",
    "    split_col: str = \"split\",\n",
    "    zero_based_rid: bool = True,\n",
    "    drop_original_rxn: bool = True,\n",
    "    encoding: str = \"utf-8\",\n",
    ") -> pd.DataFrame:\n",
    "    if isinstance(df_or_path, str):\n",
    "        df = pd.read_csv(df_or_path, encoding=encoding)\n",
    "    else:\n",
    "        df = df_or_path.copy()\n",
    "\n",
    "    if id_col in df.columns:\n",
    "        df = df.rename(columns={id_col: \"source\"})\n",
    "    else:\n",
    "        df[\"source\"] = df.index.astype(str)\n",
    "\n",
    "    if rxn_col not in df.columns:\n",
    "        raise KeyError(f\"Reaction column '{rxn_col}' not found.\")\n",
    "\n",
    "    start = 0 if zero_based_rid else 1\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"R-id\"] = [f\"uspto_{i + start}\" for i in df.index]\n",
    "\n",
    "    def _split_make_rxn(cell):\n",
    "        if pd.isna(cell):\n",
    "            return None\n",
    "        s = str(cell).strip()\n",
    "        parts = [p.strip() for p in s.split(\">\")]\n",
    "        if len(parts) >= 3:\n",
    "            reactants = parts[0] or \"\"\n",
    "            production = \">\".join(p for p in parts[2:] if p != \"\") or \"\"\n",
    "        elif len(parts) == 2:\n",
    "            reactants = parts[0] or \"\"\n",
    "            production = parts[1] or \"\"\n",
    "        else:\n",
    "            reactants = parts[0] or \"\"\n",
    "            production = \"\"\n",
    "        if reactants == \"\" and production == \"\":\n",
    "            return None\n",
    "        return f\"{reactants}>>{production}\"\n",
    "\n",
    "    df[\"aam\"] = df[rxn_col].apply(_split_make_rxn)\n",
    "\n",
    "    if split_col in df.columns:\n",
    "        out_split = df[split_col].astype(object)\n",
    "    else:\n",
    "        out_split = pd.Series([None] * len(df), name=\"split\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"R-id\": df[\"R-id\"],\n",
    "        \"aam\": df[\"aam\"],\n",
    "        \"split\": out_split,\n",
    "        \"source\": df[\"source\"],\n",
    "    })\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropbox_url = \"https://www.dropbox.com/scl/fo/df10x2546d7a0483tousa/AGhjiD7hSUY4AmQJd3DrUQE/USPTO_50K_data?dl=0&rlkey=n2s3kn34bnfkzkmii4jeb9woy&subfolder_nav_tracking=1\"\n",
    "df = download_and_combine_raw_splits(dropbox_url, save_csv=None)\n",
    "print(df.shape)\n",
    "print(df[\"split\"].value_counts())\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import traceback\n",
    "from typing import Tuple, Optional, Any, List, Dict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "_WORKER_STD = None\n",
    "_WORKER_CANON = None\n",
    "\n",
    "def _create_worker_instances(std_factory=None, canon_factory=None):\n",
    "    \"\"\"Create or return cached Standardize/CanonRSMI instances inside a worker.\"\"\"\n",
    "    global _WORKER_STD, _WORKER_CANON\n",
    "    if _WORKER_STD is None or _WORKER_CANON is None:\n",
    "        if std_factory is not None and canon_factory is not None:\n",
    "            _WORKER_STD = std_factory()\n",
    "            _WORKER_CANON = canon_factory()\n",
    "        else:\n",
    "            # default lazy import/construct\n",
    "            try:\n",
    "                from synkit.Chem.Reaction.standardize import Standardize\n",
    "                from synkit.Chem.Reaction.canon_rsmi import CanonRSMI\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\"Failed to import Standardize/CanonRSMI in worker: \" + str(e))\n",
    "            _WORKER_STD = Standardize()\n",
    "            _WORKER_CANON = CanonRSMI()\n",
    "    return _WORKER_STD, _WORKER_CANON\n",
    "\n",
    "\n",
    "def _canonicalise_value_worker_v2(idx, original_value, aam_col,\n",
    "                                  std_factory, canon_factory):\n",
    "    \"\"\"\n",
    "    Worker function: apply std.fit(..., remove_aam=False) then canon.canonicalise(...).canonical_rsmi\n",
    "    Returns: (idx, canonical_string_or_None, error_or_None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        std, canon = _create_worker_instances(std_factory=std_factory, canon_factory=canon_factory)\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc(limit=6)\n",
    "        return idx, None, f\"worker init/import error: {e}\\n{tb}\"\n",
    "\n",
    "    def _to_str_or_none(x: Any) -> Optional[str]:\n",
    "        if x is None:\n",
    "            return None\n",
    "        if isinstance(x, float) and pd.isna(x):\n",
    "            return None\n",
    "        s = str(x).strip()\n",
    "        return s if s != \"\" else None\n",
    "\n",
    "    try:\n",
    "        s = _to_str_or_none(original_value)\n",
    "        if s is None:\n",
    "            return idx, None, \"empty_or_nan\"\n",
    "\n",
    "        # IMPORTANT: use remove_aam=False per user's correct code\n",
    "        try:\n",
    "            fitted = std.fit(s, remove_aam=False)\n",
    "        except TypeError:\n",
    "            # in case older/newer API has different signature, try fallback without kwargs\n",
    "            try:\n",
    "                fitted = std.fit(s)\n",
    "            except Exception as e:\n",
    "                tb = traceback.format_exc(limit=6)\n",
    "                return idx, None, f\"std.fit error: {e}\\n{tb}\"\n",
    "        except Exception as e:\n",
    "            tb = traceback.format_exc(limit=6)\n",
    "            return idx, None, f\"std.fit error: {e}\\n{tb}\"\n",
    "\n",
    "        # decide candidate to pass to canonicalise\n",
    "        cand = None\n",
    "        if isinstance(fitted, str):\n",
    "            cand = fitted\n",
    "        else:\n",
    "            # try common attr names\n",
    "            for attr in (\"canonical_rsmi\", \"rsmi\", \"rxn\", \"reaction\", \"smiles\"):\n",
    "                if hasattr(fitted, attr):\n",
    "                    val = getattr(fitted, attr)\n",
    "                    if isinstance(val, str) and val.strip():\n",
    "                        cand = val.strip()\n",
    "                        break\n",
    "            if cand is None:\n",
    "                # fallback to string representation\n",
    "                cand = str(fitted).strip() if str(fitted).strip() else None\n",
    "\n",
    "        if not cand:\n",
    "            return idx, None, \"empty_after_std\"\n",
    "\n",
    "        # canonicalise\n",
    "        try:\n",
    "            canon_out = canon.canonicalise(cand)\n",
    "        except Exception as e:\n",
    "            tb = traceback.format_exc(limit=6)\n",
    "            return idx, None, f\"canon.canonicalise error: {e}\\n{tb}\"\n",
    "\n",
    "        # extract canonical_rsmi preferentially\n",
    "        canonical_string = None\n",
    "        if isinstance(canon_out, str):\n",
    "            canonical_string = canon_out.strip()\n",
    "        else:\n",
    "            if hasattr(canon_out, \"canonical_rsmi\"):\n",
    "                val = getattr(canon_out, \"canonical_rsmi\")\n",
    "                if isinstance(val, str) and val.strip():\n",
    "                    canonical_string = val.strip()\n",
    "            # tolerant fallback: try other attrs if canonical_rsmi missing\n",
    "            if canonical_string is None:\n",
    "                for attr in (\"canonical\", \"canonical_smiles\", \"rsmi\", \"r_smiles\"):\n",
    "                    if hasattr(canon_out, attr):\n",
    "                        val = getattr(canon_out, attr)\n",
    "                        if isinstance(val, str) and val.strip():\n",
    "                            canonical_string = val.strip()\n",
    "                            break\n",
    "            if canonical_string is None:\n",
    "                # final fallback to str()\n",
    "                srep = str(canon_out).strip()\n",
    "                canonical_string = srep if srep else None\n",
    "\n",
    "        if not canonical_string:\n",
    "            return idx, None, \"empty_after_canonicalise\"\n",
    "\n",
    "        return idx, canonical_string, None\n",
    "\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc(limit=6)\n",
    "        return idx, None, f\"unexpected error: {e}\\n{tb}\"\n",
    "\n",
    "\n",
    "def fix_aam(\n",
    "    df: pd.DataFrame,\n",
    "    std=None,\n",
    "    canon=None,\n",
    "    aam_col: str = \"aam\",\n",
    "    out_col: Optional[str] = None,\n",
    "    overwrite: bool = True,\n",
    "    show_progress: bool = False,\n",
    "    stop_on_error: bool = False,\n",
    "    n_jobs: int = 1,\n",
    "    backend: str = \"loky\",\n",
    "    std_factory=None,\n",
    "    canon_factory=None,\n",
    "    batch_size: int = 1,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Parallel canonicalisation using std.fit(..., remove_aam=False) and canon.canonicalise(...).canonical_rsmi.\n",
    "\n",
    "    - If n_jobs==1, uses provided std/canon instances (or imports locally).\n",
    "    - If n_jobs!=1, workers instantiate std/canon themselves via std_factory/canon_factory or default import.\n",
    "    - Returns (out_df, errors_df).\n",
    "    \"\"\"\n",
    "    if out_col is None:\n",
    "        out_col = aam_col if overwrite else f\"{aam_col}_fixed\"\n",
    "\n",
    "    if aam_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{aam_col}' not found in DataFrame\")\n",
    "\n",
    "    # Serial fast path (n_jobs == 1)\n",
    "    if n_jobs == 1:\n",
    "        # ensure std/canon available\n",
    "        if std is None or canon is None:\n",
    "            try:\n",
    "                from synkit.Chem.Reaction.standardize import Standardize\n",
    "                from synkit.Chem.Reaction.canon_rsmi import CanonRSMI\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\"Failed to import Standardize/CanonRSMI locally: \" + str(e))\n",
    "            std = std or Standardize()\n",
    "            canon = canon or CanonRSMI()\n",
    "\n",
    "        def _canonicalise_one_serial(original_value):\n",
    "            try:\n",
    "                if original_value is None or (isinstance(original_value, float) and pd.isna(original_value)):\n",
    "                    return None, \"empty_or_nan\"\n",
    "                s = str(original_value).strip()\n",
    "                if s == \"\":\n",
    "                    return None, \"empty_or_nan\"\n",
    "\n",
    "                try:\n",
    "                    fitted = std.fit(s, remove_aam=False)\n",
    "                except TypeError:\n",
    "                    fitted = std.fit(s)\n",
    "                except Exception as e:\n",
    "                    tb = traceback.format_exc(limit=6)\n",
    "                    return None, f\"std.fit error: {e}\\n{tb}\"\n",
    "\n",
    "                cand = None\n",
    "                if isinstance(fitted, str):\n",
    "                    cand = fitted\n",
    "                else:\n",
    "                    for attr in (\"canonical_rsmi\", \"rsmi\", \"rxn\", \"reaction\", \"smiles\"):\n",
    "                        if hasattr(fitted, attr):\n",
    "                            val = getattr(fitted, attr)\n",
    "                            if isinstance(val, str) and val.strip():\n",
    "                                cand = val.strip()\n",
    "                                break\n",
    "                    if cand is None:\n",
    "                        cand = str(fitted).strip() if str(fitted).strip() else None\n",
    "\n",
    "                if not cand:\n",
    "                    return None, \"empty_after_std\"\n",
    "\n",
    "                try:\n",
    "                    canon_out = canon.canonicalise(cand)\n",
    "                except Exception as e:\n",
    "                    tb = traceback.format_exc(limit=6)\n",
    "                    return None, f\"canon.canonicalise error: {e}\\n{tb}\"\n",
    "\n",
    "                canonical_string = None\n",
    "                if hasattr(canon_out, \"canonical_rsmi\"):\n",
    "                    val = getattr(canon_out, \"canonical_rsmi\")\n",
    "                    if isinstance(val, str) and val.strip():\n",
    "                        canonical_string = val.strip()\n",
    "                if canonical_string is None:\n",
    "                    for attr in (\"canonical\", \"canonical_smiles\", \"rsmi\", \"r_smiles\"):\n",
    "                        if hasattr(canon_out, attr):\n",
    "                            val = getattr(canon_out, attr)\n",
    "                            if isinstance(val, str) and val.strip():\n",
    "                                canonical_string = val.strip()\n",
    "                                break\n",
    "                if canonical_string is None:\n",
    "                    sval = str(canon_out).strip()\n",
    "                    canonical_string = sval if sval else None\n",
    "\n",
    "                if not canonical_string:\n",
    "                    return None, \"empty_after_canonicalise\"\n",
    "                return canonical_string, None\n",
    "            except Exception as e:\n",
    "                tb = traceback.format_exc(limit=6)\n",
    "                return None, f\"unexpected error: {e}\\n{tb}\"\n",
    "\n",
    "        out_df = df.copy()\n",
    "        errors: List[Dict] = []\n",
    "        for idx in df.index:\n",
    "            original = out_df.at[idx, aam_col]\n",
    "            fixed, err = _canonicalise_one_serial(original)\n",
    "            out_df.at[idx, out_col] = fixed\n",
    "            if err is not None:\n",
    "                errors.append({\n",
    "                    \"index\": idx,\n",
    "                    \"R-id\": out_df.at[idx, \"R-id\"] if \"R-id\" in out_df.columns else None,\n",
    "                    \"source\": out_df.at[idx, \"source\"] if \"source\" in out_df.columns else None,\n",
    "                    \"original\": original,\n",
    "                    \"error\": err,\n",
    "                })\n",
    "                if stop_on_error:\n",
    "                    raise RuntimeError(f\"Row {idx} failed: {err}; original={original}\")\n",
    "        errors_df = pd.DataFrame(errors)\n",
    "        if overwrite and out_col != aam_col:\n",
    "            out_df[aam_col] = out_df[out_col]\n",
    "            out_df.drop(columns=[out_col], inplace=True)\n",
    "        return out_df, errors_df\n",
    "\n",
    "    # Parallel path (n_jobs != 1)\n",
    "    tasks = [(int(idx), df.at[idx, aam_col]) for idx in df.index]\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, backend=backend, batch_size=batch_size)(\n",
    "        delayed(_canonicalise_value_worker_v2)(idx, original, aam_col, std_factory, canon_factory)\n",
    "        for idx, original in tasks\n",
    "    )\n",
    "\n",
    "    out_df = df.copy()\n",
    "    errors: List[Dict] = []\n",
    "    for idx, canonical_string, err in results:\n",
    "        out_df.at[idx, out_col] = canonical_string\n",
    "        if err is not None:\n",
    "            errors.append({\n",
    "                \"index\": idx,\n",
    "                \"R-id\": out_df.at[idx, \"R-id\"] if \"R-id\" in out_df.columns else None,\n",
    "                \"source\": out_df.at[idx, \"source\"] if \"source\" in out_df.columns else None,\n",
    "                \"original\": df.at[idx, aam_col],\n",
    "                \"error\": err,\n",
    "            })\n",
    "\n",
    "    errors_df = pd.DataFrame(errors)\n",
    "\n",
    "    if stop_on_error and not errors_df.empty:\n",
    "        sample = errors_df.iloc[0].to_dict()\n",
    "        raise RuntimeError(f\"Errors occurred during parallel processing; sample error: {sample}\")\n",
    "\n",
    "    if overwrite and out_col != aam_col:\n",
    "        out_df[aam_col] = out_df[out_col]\n",
    "        out_df.drop(columns=[out_col], inplace=True)\n",
    "\n",
    "    return out_df, errors_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synkit.Chem.Reaction.standardize import Standardize\n",
    "from synkit.Chem.Reaction.canon_rsmi import CanonRSMI\n",
    "std = Standardize()\n",
    "canon = CanonRSMI()\n",
    "df = curate_uspto_minimal(df)\n",
    "df_fixed, errors_df = fix_aam(df.iloc[:,:], std=std, canon=canon, n_jobs=4, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synrxn.io.io import save_df_gz\n",
    "save_df_gz(df_fixed, '../Data/synthesis/uspto_50k.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. USPTO MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import re\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "RC_PATTERN = re.compile(r'^\\d+-\\d+(?:;\\d+-\\d+)*$')\n",
    "\n",
    "def _parse_rc_string_to_tuples(rc: str, zero_index: bool = False) -> Optional[List[Tuple[int,int]]]:\n",
    "    \"\"\"'15-19;6-15;6-8' -> [(15,19),(6,15),(6,8)] (or zero-indexed if zero_index=True).\"\"\"\n",
    "    if not rc:\n",
    "        return None\n",
    "    parts = rc.split(';')\n",
    "    parsed = []\n",
    "    for p in parts:\n",
    "        if '-' not in p:\n",
    "            return None\n",
    "        a, b = p.split('-', 1)\n",
    "        try:\n",
    "            ai, bi = int(a), int(b)\n",
    "        except ValueError:\n",
    "            return None\n",
    "        if zero_index:\n",
    "            ai -= 1\n",
    "            bi -= 1\n",
    "        parsed.append((ai, bi))\n",
    "    return parsed\n",
    "\n",
    "def process_uspto_mt(\n",
    "    url: str,\n",
    "    files_map: Optional[Dict[str, str]] = None,\n",
    "    *,\n",
    "    encoding: str = \"utf-8\",\n",
    "    strip_lines: bool = True,\n",
    "    zero_index: bool = False,\n",
    "    save_csv: Optional[str] = None,\n",
    "    timeout: int = 30,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download ZIP at `url`, extract split files and return DataFrame with columns:\n",
    "      - rxn : reaction string (line content without trailing rc token)\n",
    "      - split: 'train'|'valid'|'test'\n",
    "      - rc  : parsed list of (int,int) tuples (e.g. [(15,19),(6,15)]) or None\n",
    "\n",
    "    Notes:\n",
    "      - If a trailing whitespace-separated token matches the RC pattern (N-N[;N-N...]),\n",
    "        it will be parsed and stored in `rc` as tuples. Otherwise `rc` is None.\n",
    "      - Set zero_index=True if you prefer rc indices to be zero-based.\n",
    "    \"\"\"\n",
    "    if files_map is None:\n",
    "        files_map = {\"train\": \"train.txt\", \"valid\": \"valid.txt\", \"test\": \"test.txt\"}\n",
    "\n",
    "    resp = requests.get(url, stream=True, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    zbytes = io.BytesIO(resp.content)\n",
    "\n",
    "    def _find_member(zf: zipfile.ZipFile, target: str) -> Optional[str]:\n",
    "        nm_list = zf.namelist()\n",
    "        if target in nm_list:\n",
    "            return target\n",
    "        low = target.lower()\n",
    "        for nm in nm_list:\n",
    "            if nm.lower().endswith(low):\n",
    "                return nm\n",
    "        for nm in nm_list:\n",
    "            if low in nm.lower():\n",
    "                return nm\n",
    "        return None\n",
    "\n",
    "    rows = []\n",
    "    with zipfile.ZipFile(zbytes) as zf:\n",
    "        for split, desired_name in files_map.items():\n",
    "            member = _find_member(zf, desired_name)\n",
    "            if member is None:\n",
    "                print(f\"Warning: '{desired_name}' not found in archive; skipping split '{split}'.\")\n",
    "                continue\n",
    "            with zf.open(member) as fh:\n",
    "                text = fh.read().decode(encoding, errors=\"replace\")\n",
    "\n",
    "            for line in text.splitlines():\n",
    "                if strip_lines:\n",
    "                    line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                parts = line.rsplit(None, 1)  # split on last whitespace token\n",
    "                rc_parsed = None\n",
    "                if len(parts) == 2 and RC_PATTERN.fullmatch(parts[1]):\n",
    "                    rc_parsed = _parse_rc_string_to_tuples(parts[1], zero_index=zero_index)\n",
    "                    rxn = parts[0]\n",
    "                else:\n",
    "                    rxn = line\n",
    "                    rc_parsed = None\n",
    "\n",
    "                rows.append({\"aam\": rxn, \"split\": split, \"rc\": rc_parsed})\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No data found in ZIP for any requested split files.\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if save_csv:\n",
    "        df.to_csv(save_csv, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/wengong-jin/nips17-rexgen/raw/refs/heads/master/USPTO/data.zip\"\n",
    "df = process_uspto_mt(url)\n",
    "print(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fixed, errors_df = fix_aam(df.iloc[:,:], std=std, canon=canon, n_jobs=6, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_gz(df_fixed, '../Data/synthesis/uspto_mit.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USPTO_500_MT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "import io\n",
    "import tarfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def load_df_from_tar_url(\n",
    "    url: str,\n",
    "    sep: Optional[str] = None,\n",
    "    names: Optional[list] = None,\n",
    "    encoding: str = \"utf-8\",\n",
    "    timeout: int = 60,\n",
    "    max_in_memory_bytes: int = 300 * 1024 * 1024,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    resp = requests.get(url, stream=True, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    results: Dict[str, pd.DataFrame] = {}\n",
    "    with tarfile.open(fileobj=resp.raw, mode=\"r|bz2\") as tf:\n",
    "        for member in tf:\n",
    "            if not member.isreg():\n",
    "                continue\n",
    "            member_name = member.name\n",
    "            fh = tf.extractfile(member)\n",
    "            if fh is None:\n",
    "                continue\n",
    "            try:\n",
    "                b = fh.read()\n",
    "            except Exception:\n",
    "                continue\n",
    "            if len(b) > max_in_memory_bytes:\n",
    "                pass\n",
    "            try:\n",
    "                text = b.decode(encoding)\n",
    "            except Exception:\n",
    "                text = b.decode(encoding, errors=\"replace\")\n",
    "            stream = io.StringIO(text)\n",
    "            try:\n",
    "                if names is not None:\n",
    "                    df = pd.read_csv(stream, header=None, names=names, sep=sep)\n",
    "                else:\n",
    "                    if sep is None:\n",
    "                        df = pd.read_csv(stream, sep=None, engine=\"python\", low_memory=False)\n",
    "                    else:\n",
    "                        df = pd.read_csv(stream, sep=sep, low_memory=False)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    stream.seek(0)\n",
    "                    df = pd.read_csv(stream, header=None, names=names or [\"col0\"], sep=sep or r\"\\s+\", engine=\"python\")\n",
    "                except Exception:\n",
    "                    continue\n",
    "            results[member_name] = df\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://yzhang.hpc.nyu.edu/T5Chem/data/USPTO_500_MT.tar.bz2\"\n",
    "\n",
    "# load the first CSV whose path contains \"train\" (substring match)\n",
    "dfs = load_df_from_tar_url(URL)\n",
    "# `dfs` will be { 'path/inside/archive/train.csv': DataFrame(...) }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "import io\n",
    "import tarfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_reagents_from_tar_url(\n",
    "    url: str,\n",
    "    sep: Optional[str] = None,\n",
    "    names: Optional[list] = None,\n",
    "    encoding: str = \"utf-8\",\n",
    "    timeout: int = 60,\n",
    "    max_in_memory_bytes: int = 300 * 1024 * 1024,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    resp = requests.get(url, stream=True, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    results: Dict[str, pd.DataFrame] = {}\n",
    "    with tarfile.open(fileobj=resp.raw, mode=\"r|bz2\") as tf:\n",
    "        for member in tf:\n",
    "            if not member.isreg():\n",
    "                continue\n",
    "            member_name = member.name\n",
    "            if not member_name.startswith(\"data/USPTO_500_MT/Reagents/\"):\n",
    "                continue\n",
    "            fh = tf.extractfile(member)\n",
    "            if fh is None:\n",
    "                continue\n",
    "            try:\n",
    "                b = fh.read()\n",
    "            except Exception:\n",
    "                continue\n",
    "            if len(b) > max_in_memory_bytes:\n",
    "                pass\n",
    "            try:\n",
    "                text = b.decode(encoding)\n",
    "            except Exception:\n",
    "                text = b.decode(encoding, errors=\"replace\")\n",
    "            stream = io.StringIO(text)\n",
    "            try:\n",
    "                if names is not None:\n",
    "                    df = pd.read_csv(stream, header=None, names=names, sep=sep)\n",
    "                else:\n",
    "                    if sep is None:\n",
    "                        df = pd.read_csv(stream, sep=None, engine=\"python\", low_memory=False)\n",
    "                    else:\n",
    "                        df = pd.read_csv(stream, sep=sep, low_memory=False)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    stream.seek(0)\n",
    "                    df = pd.read_csv(stream, header=None, names=names or [\"col0\"], sep=sep or r\"\\s+\", engine=\"python\")\n",
    "                except Exception:\n",
    "                    continue\n",
    "            results[member_name] = df\n",
    "    return results\n",
    "\n",
    "def combine_reagents_dict(dfs: Dict[str, pd.DataFrame], prefix: str = \"data/USPTO_500_MT/Reagents/\") -> pd.DataFrame:\n",
    "    mapping = {}\n",
    "    for k, df in dfs.items():\n",
    "        name = k[len(prefix):] if k.startswith(prefix) else k.split(\"/\")[-1]\n",
    "        parts = name.split(\".\")\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        split, role = parts[0], parts[1]\n",
    "        mapping.setdefault(split, {})[role] = df.reset_index(drop=True)\n",
    "\n",
    "    rows = []\n",
    "    for split, grp in mapping.items():\n",
    "        src_df = grp.get(\"source\")\n",
    "        tgt_df = grp.get(\"target\")\n",
    "        def col_series(df):\n",
    "            if df is None:\n",
    "                return None\n",
    "            return df.iloc[:, 0].astype(str).str.strip().reset_index(drop=True)\n",
    "        src_s = col_series(src_df)\n",
    "        tgt_s = col_series(tgt_df)\n",
    "        n = max(0, (len(src_s) if src_s is not None else 0), (len(tgt_s) if tgt_s is not None else 0))\n",
    "        for i in range(n):\n",
    "            rxn = src_s.iloc[i] if (src_s is not None and i < len(src_s)) else np.nan\n",
    "            reagent = tgt_s.iloc[i] if (tgt_s is not None and i < len(tgt_s)) else np.nan\n",
    "            rows.append({\"rxn\": rxn, \"reagent\": reagent, \"split\": split})\n",
    "    return pd.DataFrame(rows)[[\"rxn\", \"reagent\", \"split\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://yzhang.hpc.nyu.edu/T5Chem/data/USPTO_500_MT.tar.bz2\"\n",
    "data = load_reagents_from_tar_url(URL)\n",
    "results = combine_reagents_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = combine_reagents_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnxdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
