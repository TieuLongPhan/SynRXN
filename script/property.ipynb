{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from synrxn.io.io import save_df_gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Iterable, Optional, List\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def curate_reactions(\n",
    "    data: Union[str, pd.DataFrame],\n",
    "    *,\n",
    "    data_name: str,\n",
    "    rxn_col: str,\n",
    "    target_cols: Union[str, Iterable[str]],\n",
    "    split_col: Optional[Union[str, Iterable[str]]] = None,\n",
    "    r_id_col: str = \"R-id\",\n",
    "    index_base: int = 0,\n",
    "    index_zero_pad: Optional[int] = None,\n",
    "    keep_other_columns: bool = False,\n",
    "    inplace: bool = False,\n",
    "    out_csv: Optional[Union[str, Path]] = None,\n",
    "    encoding: str = \"utf-8\",\n",
    "    check_unique_rid: bool = True,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    if isinstance(data, str) or isinstance(data, Path):\n",
    "        df_in = pd.read_csv(str(data), encoding=encoding)\n",
    "        orig_is_df = False\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        df_in = data if inplace else data.copy(deep=True)\n",
    "        orig_is_df = True\n",
    "    else:\n",
    "        raise TypeError(\"`data` must be a file path or a pandas DataFrame\")\n",
    "\n",
    "    if isinstance(target_cols, str):\n",
    "        target_list: List[str] = [target_cols]\n",
    "    else:\n",
    "        target_list = list(target_cols)\n",
    "\n",
    "    if split_col is None:\n",
    "        split_list: List[str] = []\n",
    "    elif isinstance(split_col, str):\n",
    "        split_list = [split_col]\n",
    "    else:\n",
    "        split_list = list(split_col)\n",
    "\n",
    "    if not isinstance(index_base, int) or index_base < 0:\n",
    "        raise ValueError(\"index_base must be a non-negative integer\")\n",
    "\n",
    "    expected_cols = [rxn_col] + target_list + split_list\n",
    "    missing = [c for c in expected_cols if c not in df_in.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing expected column(s) in input data: {missing}\")\n",
    "\n",
    "    df_in.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    idx_vals = (df_in.index + index_base).astype(int).astype(str)\n",
    "    if index_zero_pad is not None:\n",
    "        if not isinstance(index_zero_pad, int) or index_zero_pad <= 0:\n",
    "            raise ValueError(\"index_zero_pad must be a positive integer or None\")\n",
    "        idx_vals = idx_vals.str.zfill(index_zero_pad)\n",
    "\n",
    "    rids = data_name + \"_\" + idx_vals\n",
    "    df_in[r_id_col] = rids\n",
    "\n",
    "    if rxn_col != \"rxn\":\n",
    "        if \"rxn\" in df_in.columns and rxn_col != \"rxn\":\n",
    "            df_in.rename(columns={\"rxn\": \"rxn_orig\"}, inplace=True)\n",
    "            if verbose:\n",
    "                print(\"Renamed existing 'rxn' column to 'rxn_orig' to avoid collision.\")\n",
    "        df_in = df_in.rename(columns={rxn_col: \"rxn\"})\n",
    "\n",
    "    df_in[\"rxn\"] = df_in[\"rxn\"].astype(str).str.strip()\n",
    "\n",
    "    keep_cols = [r_id_col, \"rxn\"] + target_list + split_list\n",
    "\n",
    "    if keep_other_columns:\n",
    "        other_cols = [c for c in df_in.columns if c not in keep_cols]\n",
    "        ordered_cols = keep_cols + other_cols\n",
    "        result = df_in.loc[:, ordered_cols]\n",
    "    else:\n",
    "        result = df_in.loc[:, [c for c in keep_cols if c in df_in.columns]]\n",
    "\n",
    "    if check_unique_rid:\n",
    "        if result[r_id_col].duplicated().any():\n",
    "            dupes = result[result[r_id_col].duplicated(keep=False)][r_id_col].unique().tolist()\n",
    "            raise ValueError(f\"Non-unique R-id values produced (sample): {dupes[:10]}\")\n",
    "\n",
    "    if out_csv:\n",
    "        result.to_csv(str(out_csv), index=False, encoding=encoding)\n",
    "        if verbose:\n",
    "            print(f\"Wrote curated DataFrame to {out_csv}\")\n",
    "\n",
    "    if inplace and orig_is_df:\n",
    "        orig_df = data  # type: ignore[assignment]\n",
    "        for col in list(orig_df.columns):\n",
    "            orig_df.drop(columns=col, inplace=True)\n",
    "        for col in result.columns:\n",
    "            orig_df[col] = result[col].values\n",
    "        return orig_df\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. B97xd3\n",
    "https://www.nature.com/articles/s41597-020-0460-4#Sec2\n",
    "\n",
    "https://doi.org/10.5281/zenodo.3715478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b97xd3 = \"https://zenodo.org/records/3715478/files/b97d3.csv?download=1\"\n",
    "b97xd3 = pd.read_csv(b97xd3)\n",
    "b97xd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(r, p):\n",
    "    return f\"{r}>>{p}\"\n",
    "\n",
    "b97xd3['rxn'] = b97xd3.apply(lambda row: combine(row['rsmi'], row['psmi']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b97xd3 = curate_reactions(\n",
    "    b97xd3,\n",
    "    data_name=\"b97xd3\",\n",
    "    rxn_col=\"rxn\",\n",
    "    target_cols=['ea', 'dh'],\n",
    "    split_col=None,\n",
    "    index_base=1,               \n",
    "    keep_other_columns=False,   \n",
    ")\n",
    "\n",
    "save_df_gz(b97xd3, '../Data/property/b97xd3.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b97xd3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SnAR\n",
    "https://pubs.rsc.org/en/content/articlelanding/2021/sc/d0sc04896h\n",
    "\n",
    "https://www.rsc.org/suppdata/d0/sc/d0sc04896h/d0sc04896h2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import csv\n",
    "import zipfile\n",
    "import tempfile\n",
    "from typing import Optional\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_snar_df(\n",
    "    url: str = \"https://www.rsc.org/suppdata/d0/sc/d0sc04896h/d0sc04896h2.zip\",\n",
    "    target_basename: str = \"SNAR_reaction_dataset_SI.csv\",\n",
    "    timeout: int = 30,\n",
    ") -> pd.DataFrame:\n",
    "    encodings_default = [\"cp1252\", \"latin1\", \"iso-8859-1\", \"utf-8\", \"utf-16\"]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        zip_path = os.path.join(td, \"archive.zip\")\n",
    "\n",
    "        with requests.get(url, stream=True, timeout=timeout) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(zip_path, \"wb\") as fh:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        fh.write(chunk)\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            names = z.namelist()\n",
    "            candidate: Optional[str] = None\n",
    "            for n in names:\n",
    "                if target_basename.lower() in os.path.basename(n).lower():\n",
    "                    candidate = n\n",
    "                    break\n",
    "            if candidate is None:\n",
    "                for n in names:\n",
    "                    if target_basename.lower() in n.lower():\n",
    "                        candidate = n\n",
    "                        break\n",
    "            if candidate is None:\n",
    "                sample = names[:40]\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Could not find a file matching {target_basename!r} inside the ZIP. Sample entries: {sample}\"\n",
    "                )\n",
    "\n",
    "            raw_bytes = z.read(candidate)\n",
    "            sample_bytes = raw_bytes[:200_000]\n",
    "\n",
    "            detected_enc = None\n",
    "            try:\n",
    "                import chardet  # type: ignore\n",
    "                det = chardet.detect(sample_bytes)\n",
    "                detected_enc = det.get(\"encoding\")\n",
    "            except Exception:\n",
    "                detected_enc = None\n",
    "\n",
    "            encodings = []\n",
    "            if detected_enc:\n",
    "                encodings.append(detected_enc)\n",
    "            for e in encodings_default:\n",
    "                if e not in encodings:\n",
    "                    encodings.append(e)\n",
    "\n",
    "            delim = \",\"\n",
    "            for enc in encodings:\n",
    "                try:\n",
    "                    sample_text = sample_bytes.decode(enc, errors=\"replace\")\n",
    "                    sniff = csv.Sniffer()\n",
    "                    dialect = sniff.sniff(sample_text)\n",
    "                    delim = dialect.delimiter\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            last_exc: Optional[Exception] = None\n",
    "            for enc in encodings:\n",
    "                try:\n",
    "                    df = pd.read_csv(io.BytesIO(raw_bytes), encoding=enc, delimiter=delim, engine=\"c\", low_memory=False)\n",
    "                    return df\n",
    "                except Exception as e_c:\n",
    "                    last_exc = e_c\n",
    "                    try:\n",
    "                        df = pd.read_csv(io.BytesIO(raw_bytes), encoding=enc, delimiter=delim, engine=\"python\")\n",
    "                        return df\n",
    "                    except Exception as e_py:\n",
    "                        last_exc = e_py\n",
    "                        continue\n",
    "\n",
    "            text = raw_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "            try:\n",
    "                df = pd.read_csv(io.StringIO(text), delimiter=delim)\n",
    "                return df\n",
    "            except Exception as final_e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Failed to parse CSV inside ZIP with multiple encodings and fallbacks. Last parsing error: {last_exc!r}. Final attempt error: {final_e!r}\"\n",
    "                ) from final_e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snar = fetch_snar_df()\n",
    "snar['ea'] = snar['Activation Free Energy (kcalmol-1)']\n",
    "snar = curate_reactions(\n",
    "    snar,\n",
    "    data_name=\"snar\",\n",
    "    rxn_col=\"Reaction SMILES\",\n",
    "    target_cols='ea',\n",
    "    split_col=None,\n",
    "    index_base=1,               \n",
    "    keep_other_columns=False,   \n",
    ")\n",
    "save_df_gz(snar, '../Data/property/snar.csv.gz')\n",
    "print(snar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. E2SN2\n",
    "https://doi.org/10.1088/2632-2153/aba822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2sn2 ='https://raw.githubusercontent.com/hesther/reactiondatabase/refs/heads/main/data/e2sn2.csv'\n",
    "\n",
    "e2sn2 = pd.read_csv(e2sn2)\n",
    "e2sn2 = curate_reactions(\n",
    "    e2sn2,\n",
    "    data_name=\"e2sn2\",\n",
    "    rxn_col=\"AAM\",\n",
    "    target_cols='ea',\n",
    "    split_col=None,\n",
    "    index_base=1,               \n",
    "    keep_other_columns=False,   \n",
    ")\n",
    "save_df_gz(e2sn2, '../Data/property/e2sn2.csv.gz')\n",
    "print(e2sn2.shape)\n",
    "display(e2sn2.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Rad6re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad6re = 'https://github.com/hesther/reactiondatabase/raw/refs/heads/main/data/rad6re.csv'\n",
    "\n",
    "rad6re = pd.read_csv(rad6re)\n",
    "rad6re = curate_reactions(\n",
    "    rad6re,\n",
    "    data_name=\"rad6re\",\n",
    "    rxn_col=\"AAM\",\n",
    "    target_cols='dh',\n",
    "    split_col=None,\n",
    "    index_base=1,               \n",
    "    keep_other_columns=False,   \n",
    ")\n",
    "save_df_gz(rad6re, '../Data/property/rad6re.csv.gz')\n",
    "print(rad6re.shape)\n",
    "display(rad6re.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. lograte\n",
    "\n",
    "https://doi.org/10.1021/acs.jpca.7b07361\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lograte = 'https://raw.githubusercontent.com/hesther/reactiondatabase/refs/heads/main/data/lograte.csv'\n",
    "\n",
    "lograte = pd.read_csv(lograte)\n",
    "\n",
    "lograte = curate_reactions(\n",
    "    lograte,\n",
    "    data_name=\"lograte\",\n",
    "    rxn_col=\"AAM\",\n",
    "    target_cols='lograte',\n",
    "    split_col=None,\n",
    "    index_base=1,               \n",
    "    keep_other_columns=False,   \n",
    ")\n",
    "save_df_gz(lograte, '../Data/property/lograte.csv.gz')\n",
    "print(lograte.shape)\n",
    "display(lograte.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Phosphate\n",
    "\n",
    "https://doi.org/10.1073/pnas.1423570112\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any\n",
    "\n",
    "def add_array_column(df: pd.DataFrame, arr: Any, col_name: str) -> pd.DataFrame:\n",
    "    arr_np = np.asarray(arr)\n",
    "    if arr_np.ndim == 0:\n",
    "        values = [arr_np.item()] * len(df)\n",
    "    elif arr_np.ndim == 1:\n",
    "        values = arr_np.tolist()\n",
    "    elif arr_np.ndim == 2 and arr_np.shape[1] == 1:\n",
    "        values = arr_np.ravel().tolist()\n",
    "    else:\n",
    "        values = [list(row) for row in arr_np]\n",
    "    if len(df) != len(values):\n",
    "        raise ValueError(f\"Length mismatch: df has {len(df)} rows but array has {len(values)} rows\")\n",
    "    new_df = df.copy()\n",
    "    new_df[col_name] = values\n",
    "    return new_df\n",
    "\n",
    "phosphatase = 'https://github.com/hesther/reactiondatabase/raw/refs/heads/main/data/phosphatase.csv'\n",
    "phosphatase = pd.read_csv(phosphatase)\n",
    "\n",
    "po = 'https://github.com/hesther/reactiondatabase/raw/refs/heads/main/data/phosphatase_onehotenzyme.csv'\n",
    "po = pd.read_csv(po)\n",
    "\n",
    "phosphatase = add_array_column(phosphatase, po.values, col_name='onehot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phosphatase = curate_reactions(\n",
    "    phosphatase,\n",
    "    data_name=\"phosphatase\",\n",
    "    rxn_col=\"AAM\",\n",
    "    target_cols='Conversion',\n",
    "    split_col='onehot',\n",
    "    index_base=1,               \n",
    "    keep_other_columns=False,   \n",
    ")\n",
    "\n",
    "save_df_gz(phosphatase, '../Data/property/phosphatase.csv.gz')\n",
    "print(phosphatase.shape)\n",
    "display(phosphatase.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chemprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "def _normalize_token(s: str) -> str:\n",
    "    return re.sub(r'[^0-9a-z]', '_', s.lower())\n",
    "\n",
    "def stream_extract_selected_from_targz_safe(\n",
    "    url: str,\n",
    "    targets: List[str],\n",
    "    dest_dir: str = \"extracted\",\n",
    "    timeout: int = 1000,\n",
    "    max_no_progress: int = 100000,\n",
    "    min_matches_per_target: int = 1,\n",
    ") -> Dict[str, List[Path]]:\n",
    "    \"\"\"\n",
    "    Stream-download a .tar.gz and extract only members matching tokens in `targets`.\n",
    "    Safety and robustness improvements:\n",
    "      - skips duplicate members (seen_members)\n",
    "      - tracks tokens_found and extracted files per token\n",
    "      - stops early when all tokens have at least `min_matches_per_target` files,\n",
    "        or when `max_no_progress` consecutive members produce no new matches.\n",
    "    Returns a dict mapping normalized target -> list of extracted file Paths.\n",
    "    \"\"\"\n",
    "    dest_dir_path = Path(dest_dir)\n",
    "    dest_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    norm_targets = [_normalize_token(t) for t in targets]\n",
    "    target_map = {nt: t for nt, t in zip(norm_targets, targets)}  # norm -> original\n",
    "\n",
    "    extracted_by_target: Dict[str, List[Path]] = {nt: [] for nt in norm_targets}\n",
    "    tokens_found: Set[str] = set()\n",
    "    seen_members: Set[str] = set()\n",
    "    no_progress = 0\n",
    "    total_processed = 0\n",
    "\n",
    "    with requests.get(url, stream=True, timeout=timeout) as r:\n",
    "        r.raise_for_status()\n",
    "        r.raw.decode_content = True\n",
    "        with tarfile.open(fileobj=r.raw, mode=\"r|gz\") as tar:\n",
    "            print(\"Opened remote tar.gz in streaming mode; iterating members...\")\n",
    "            for member in tar:\n",
    "                total_processed += 1\n",
    "                # basic bailouts\n",
    "                if member is None or not getattr(member, \"name\", None):\n",
    "                    continue\n",
    "                name = member.name\n",
    "                # skip duplicates (prevents repeated extraction/printing)\n",
    "                if name in seen_members:\n",
    "                    no_progress += 1\n",
    "                    if no_progress >= max_no_progress:\n",
    "                        print(f\"No progress for {max_no_progress} members; stopping early.\")\n",
    "                        break\n",
    "                    # continue reading until stop condition triggers\n",
    "                    continue\n",
    "                seen_members.add(name)\n",
    "                no_progress += 1  # will be reset if we make progress below\n",
    "\n",
    "                name_norm = _normalize_token(name)\n",
    "                basename_norm = _normalize_token(Path(name).name)\n",
    "\n",
    "                # flexible match: if any target appears in normalized path or basename\n",
    "                matched_tokens = [tok for tok in norm_targets if (tok in name_norm or tok in basename_norm)]\n",
    "                if not matched_tokens:\n",
    "                    # no match -> continue\n",
    "                    if total_processed % 1000 == 0:\n",
    "                        print(f\"Processed {total_processed} members so far; still searching...\")\n",
    "                    # keep going; no_progress remains incremented\n",
    "                    if no_progress >= max_no_progress:\n",
    "                        print(f\"No progress for {max_no_progress} members; stopping early.\")\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                # We have at least one matching token â€” reset no-progress\n",
    "                no_progress = 0\n",
    "                for tok in matched_tokens:\n",
    "                    tokens_found.add(tok)\n",
    "\n",
    "                # Extract safely (create parent dirs, prevent path traversal)\n",
    "                target_path = dest_dir_path / name\n",
    "                target_resolved = target_path.resolve(strict=False)\n",
    "                dest_resolved = dest_dir_path.resolve()\n",
    "                if not str(target_resolved).startswith(str(dest_resolved)):\n",
    "                    print(f\"Skipping unsafe member path: {name}\")\n",
    "                    continue\n",
    "                target_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                if member.isdir():\n",
    "                    target_path.mkdir(parents=True, exist_ok=True)\n",
    "                    # record directory as \"extracted\" for each matched token\n",
    "                    for tok in matched_tokens:\n",
    "                        extracted_by_target[tok].append(target_path.resolve())\n",
    "                    print(f\"Matched directory: {name}\")\n",
    "                elif member.isreg():\n",
    "                    f = tar.extractfile(member)\n",
    "                    if f is None:\n",
    "                        print(f\"Warning: could not extract file member {name}\")\n",
    "                        continue\n",
    "                    with open(target_path, \"wb\") as outfh:\n",
    "                        while True:\n",
    "                            chunk = f.read(1024 * 64)\n",
    "                            if not chunk:\n",
    "                                break\n",
    "                            outfh.write(chunk)\n",
    "                    try:\n",
    "                        os.chmod(target_path, member.mode)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    for tok in matched_tokens:\n",
    "                        extracted_by_target[tok].append(target_path.resolve())\n",
    "                    print(f\"Extracted -> {target_path}\")\n",
    "                else:\n",
    "                    # skip symlinks / special types but create small marker\n",
    "                    print(f\"Skipping non-regular member: {name}\")\n",
    "                    with open(target_path, \"w\", encoding=\"utf-8\") as outfh:\n",
    "                        outfh.write(f\"# skipped non-regular archive member: {name}\\n\")\n",
    "                    for tok in matched_tokens:\n",
    "                        extracted_by_target[tok].append(target_path.resolve())\n",
    "\n",
    "                # Early stop: if all tokens have at least min_matches_per_target files, stop\n",
    "                if all(len(extracted_by_target[tok]) >= min_matches_per_target for tok in norm_targets):\n",
    "                    print(\"All requested targets have at least\", min_matches_per_target, \"matches. Stopping early.\")\n",
    "                    break\n",
    "\n",
    "    # Final reporting\n",
    "    print(\"Streaming extraction finished. Processed members:\", total_processed)\n",
    "    for nt in norm_targets:\n",
    "        print(f\"Target '{target_map[nt]}' -> extracted {len(extracted_by_target[nt])} item(s).\")\n",
    "    return extracted_by_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://zenodo.org/records/10078142/files/data.tar.gz?download=1\"\n",
    "targets = [\n",
    "    \"barriers_e2\",\n",
    "    \"barriers_sn2\",\n",
    "    \"barriers_cycloadd\",\n",
    "    \"barriers_rdb7\",\n",
    "    # \"barriers_rgd1\", This should direct donwload and put to folder\n",
    "]\n",
    "result = stream_extract_selected_from_targz_safe(url, targets, dest_dir=\"chemprop_zenodo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterable, Optional, Tuple, Dict, List, Union\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _try_read_csv(path: Path, encodings: Optional[List[str]] = None, **pd_kwargs) -> pd.DataFrame:\n",
    "    if encodings is None:\n",
    "        encodings = [\"utf-8\", \"cp1252\", \"latin1\", \"iso-8859-1\", \"utf-16\"]\n",
    "    raw = path.read_bytes()\n",
    "    last_exc = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(io.BytesIO(raw), encoding=enc, **pd_kwargs)\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "    # final tolerant fallback\n",
    "    try:\n",
    "        txt = raw.decode(\"utf-8\", errors=\"replace\")\n",
    "        return pd.read_csv(io.StringIO(txt), **pd_kwargs)\n",
    "    except Exception as final_e:\n",
    "        raise RuntimeError(f\"Failed to read CSV {path!s}. Last error: {last_exc!r}; final attempt: {final_e!r}\")\n",
    "\n",
    "def combine_barriers_split(\n",
    "    base_dir: str = \"./extracted_zenodo_stream_safe/data/barriers_e2\",\n",
    "    patterns_split: Optional[List[tuple]] = None,\n",
    "    encodings: Optional[List[str]] = None,\n",
    "    verbose: bool = True,\n",
    "    save: bool = False,\n",
    "    out_csv: str = \"barriers_e2_combined.csv\",\n",
    "    return_splits: bool = False,\n",
    ") -> pd.DataFrame | Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Combine train/val/test CSVs under base_dir into one DataFrame with a 'split' column.\n",
    "    - save=False -> will NOT write any file to disk (default).\n",
    "    - return_splits=True -> returns (combined_df, {'train': df_t, 'val': df_v, 'test': df_te})\n",
    "    \"\"\"\n",
    "    base = Path(base_dir)\n",
    "    if patterns_split is None:\n",
    "        patterns_split = [\n",
    "            (\"train\", \"train\"),\n",
    "            (\"val\", \"val\"),\n",
    "            (\"validation\", \"val\"),\n",
    "            (\"test\", \"test\"),\n",
    "        ]\n",
    "    if encodings is None:\n",
    "        encodings = [\"utf-8\", \"cp1252\", \"latin1\", \"iso-8859-1\", \"utf-16\"]\n",
    "\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"Base directory does not exist: {base.resolve()}\")\n",
    "\n",
    "    csv_files = sorted(list(base.rglob(\"*.csv\")))\n",
    "    if verbose:\n",
    "        print(f\"Found {len(csv_files)} CSV file(s) under {base}\")\n",
    "\n",
    "    rows = []\n",
    "    files_used = 0\n",
    "    split_frames: Dict[str, List[pd.DataFrame]] = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "    for p in csv_files:\n",
    "        name_l = p.name.lower()\n",
    "        assigned_split = None\n",
    "        for token, label in patterns_split:\n",
    "            if token in name_l:\n",
    "                assigned_split = label\n",
    "                break\n",
    "\n",
    "        # check up to 3 parent directory names\n",
    "        if assigned_split is None:\n",
    "            parents_to_check = list(p.parents)[:3]\n",
    "            for parent in parents_to_check:\n",
    "                part_l = parent.name.lower()\n",
    "                for token, label in patterns_split:\n",
    "                    if token in part_l:\n",
    "                        assigned_split = label\n",
    "                        break\n",
    "                if assigned_split is not None:\n",
    "                    break\n",
    "\n",
    "        if assigned_split is None:\n",
    "            if verbose:\n",
    "                print(f\"Skipping (no split token found): {p}\")\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Loading {p}  -> split='{assigned_split}'\")\n",
    "        try:\n",
    "            df = _try_read_csv(p, encodings=encodings)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {p}: {type(e).__name__}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df[\"split\"] = assigned_split\n",
    "        try:\n",
    "            df[\"_source_file\"] = str(p.relative_to(base.parent))\n",
    "        except Exception:\n",
    "            df[\"_source_file\"] = str(p)\n",
    "\n",
    "        rows.append(df)\n",
    "        split_frames.setdefault(assigned_split, []).append(df)\n",
    "        files_used += 1\n",
    "\n",
    "    if files_used == 0:\n",
    "        raise RuntimeError(\"No CSV files were loaded/labelled. Check filenames and patterns_split.\")\n",
    "\n",
    "    combined = pd.concat(rows, ignore_index=True, sort=False)\n",
    "    combined[\"split\"] = pd.Categorical(combined[\"split\"], categories=[\"train\", \"val\", \"test\"], ordered=True)\n",
    "\n",
    "    if save:\n",
    "        combined.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "        if verbose:\n",
    "            print(f\"Saved combined CSV to: {Path(out_csv).resolve()}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Combined {files_used} file(s) -> {combined.shape[0]} rows, {combined.shape[1]} cols\")\n",
    "\n",
    "    if return_splits:\n",
    "        # merge per-split lists into DataFrames\n",
    "        per_split = {k: (pd.concat(v, ignore_index=True, sort=False) if v else pd.DataFrame()) for k, v in split_frames.items()}\n",
    "        return combined, per_split\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = combine_barriers_split('./chemprop_zenodo/data/barriers_e2')\n",
    "e2 = curate_reactions(\n",
    "    e2,\n",
    "    data_name=\"e2\",\n",
    "    rxn_col=\"AAM\",\n",
    "    target_cols=\"ea\",\n",
    "    split_col=\"split\",\n",
    "    index_base=1,               # optional: start R-id at 1\n",
    "    keep_other_columns=False,   # drop 'extra'\n",
    ")\n",
    "\n",
    "save_df_gz(e2, '../Data/property/e2.csv.gz')\n",
    "print(e2.shape)\n",
    "display(e2.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. SN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn2 = combine_barriers_split('./chemprop_zenodo/data/barriers_sn2')\n",
    "sn2 = curate_reactions(\n",
    "    sn2,\n",
    "    data_name=\"sn2\",\n",
    "    rxn_col=\"AAM\",\n",
    "    target_cols=\"ea\",\n",
    "    split_col=\"split\",\n",
    "    index_base=1,               # optional: start R-id at 1\n",
    "    keep_other_columns=False,   # drop 'extra'\n",
    ")\n",
    "save_df_gz(sn2, '../Data/property/sn2.csv.gz')\n",
    "print(sn2.shape)\n",
    "display(sn2.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. RDB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdb7 = combine_barriers_split('./chemprop_zenodo/data/barriers_rdb7')\n",
    "rdb7 = curate_reactions(\n",
    "    rdb7,\n",
    "    data_name=\"rdb7\",\n",
    "    rxn_col=\"smiles\",\n",
    "    target_cols=\"ea\",\n",
    "    split_col=\"split\",\n",
    "    index_base=1,               # optional: start R-id at 1\n",
    "    keep_other_columns=False,   # drop 'extra'\n",
    ")\n",
    "save_df_gz(rdb7, '../Data/property/rdb7.csv.gz')\n",
    "print(rdb7.shape)\n",
    "display(rdb7.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Cycloadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycloadd = combine_barriers_split('./chemprop_zenodo/data/barriers_cycloadd')\n",
    "cycloadd = curate_reactions(\n",
    "    cycloadd,\n",
    "    data_name=\"cycloadd\",\n",
    "    rxn_col=\"rxn_smiles\",\n",
    "    target_cols=[\"G_act\",\"G_r\"],\n",
    "    split_col=\"split\",\n",
    "    index_base=1,               # optional: start R-id at 1\n",
    "    keep_other_columns=False,   # drop 'extra'\n",
    ")\n",
    "save_df_gz(cycloadd, '../Data/property/cycloadd.csv.gz')\n",
    "print(cycloadd.shape)\n",
    "display(cycloadd.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Rgd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgd1 = combine_barriers_split('./chemprop_zenodo/data/barriers_rgd1')\n",
    "\n",
    "rgd1 = curate_reactions(\n",
    "    rgd1,\n",
    "    data_name=\"rgd1\",\n",
    "    rxn_col=\"smiles\",\n",
    "    target_cols=\"ea\",\n",
    "    split_col=\"split\",\n",
    "    index_base=1,               \n",
    "    keep_other_columns=False,   \n",
    ")\n",
    "\n",
    "save_df_gz(rgd1, '../Data/property/rgd1.csv.gz')\n",
    "print(rgd1.shape)\n",
    "display(rgd1.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Suzuki-Miyaura Yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, requests, pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "_RAW_BASE = \"https://raw.githubusercontent.com/reymond-group/drfp/main/data/Suzuki-Miyaura/random_splits\"\n",
    "_STD = None\n",
    "\n",
    "def _get_std():\n",
    "    global _STD\n",
    "    if _STD is None:\n",
    "        from synkit.Chem.Reaction.standardize import Standardize\n",
    "        _STD = Standardize()\n",
    "    return _STD\n",
    "\n",
    "def _standardize_one(rxns, remove_aam=False):\n",
    "    try:\n",
    "        std = _get_std()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Failed to import synkit.Standardize. Ensure synkit is installed.\") from e\n",
    "    try:\n",
    "        try:\n",
    "            out = std.fit(rxns, remove_aam=remove_aam)\n",
    "        except TypeError:\n",
    "            out = std.fit(rxns)\n",
    "        if out is None:\n",
    "            for attr in (\"canonical_rsmi\", \"rsmi\", \"canonical\", \"standardized\"):\n",
    "                if hasattr(std, attr):\n",
    "                    maybe = getattr(std, attr)\n",
    "                    if isinstance(maybe, str) and maybe:\n",
    "                        return maybe\n",
    "            return rxns\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            return \"|\".join(map(str, out))\n",
    "        return str(out)\n",
    "    except Exception:\n",
    "        return rxns\n",
    "\n",
    "def _parse_text_to_df(txt):\n",
    "    if \"\\\\t\" in txt and \"\\t\" not in txt:\n",
    "        txt = txt.replace(\"\\\\t\", \"\\t\")\n",
    "    try:\n",
    "        df = pd.read_csv(io.StringIO(txt), sep=\"\\t\", engine=\"python\", dtype=str)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(io.StringIO(txt), sep=r\"\\s+\", engine=\"python\", header=None, dtype=str)\n",
    "    if len(df.columns) == 1:\n",
    "        col0 = df.columns[0]\n",
    "        sample_vals = df[col0].astype(str).iloc[:10].tolist()\n",
    "        if any(\"\\t\" in s for s in sample_vals):\n",
    "            df = df[col0].str.split(\"\\t\", expand=True)\n",
    "        elif any(\"\\\\t\" in s for s in sample_vals):\n",
    "            fixed = df[col0].str.replace(\"\\\\t\", \"\\t\", regex=False)\n",
    "            df = fixed.str.split(\"\\t\", expand=True)\n",
    "        else:\n",
    "            df = df[col0].str.split(r\"\\s+\", expand=True)\n",
    "    df.columns = [str(c).strip().replace(\"\\\\\", \"\") for c in df.columns]\n",
    "    col_lower = [c.lower() for c in df.columns]\n",
    "    if \"rxn\" in col_lower and \"y\" in col_lower:\n",
    "        rename_map = {}\n",
    "        for c in df.columns:\n",
    "            if c.lower() == \"rxn\":\n",
    "                rename_map[c] = \"rxn\"\n",
    "            if c.lower() == \"y\":\n",
    "                rename_map[c] = \"y\"\n",
    "        df = df.rename(columns=rename_map)\n",
    "    else:\n",
    "        if df.shape[1] >= 2:\n",
    "            first_is_index = df.iloc[:,0].astype(str).str.match(r'^\\s*\\d+\\s*$').all()\n",
    "            second_looks_like_rxn = df.shape[1] >= 2 and df.iloc[:,1].astype(str).str.contains(r'[\\.\\~\\[\\]\\/\\\\\\=\\#]').any()\n",
    "            if first_is_index and second_looks_like_rxn and df.shape[1] >= 3:\n",
    "                cols = list(df.columns)\n",
    "                new_cols = [\"suzuki_index\",\"rxn\",\"y\"] + [f\"col_{i}\" for i in range(3, len(cols))]\n",
    "                df.columns = new_cols\n",
    "            else:\n",
    "                new_cols = [\"rxn\", \"y\"] + [f\"col_{i}\" for i in range(2, df.shape[1])]\n",
    "                df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "def load_and_standardize(n_splits=10, n_jobs=-1, remove_aam=False, progress=True):\n",
    "    dfs = []\n",
    "    for i in range(n_splits):\n",
    "        name = f\"random_split_{i}.tsv\"\n",
    "        url = f\"{_RAW_BASE}/{name}\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            txt = r.text\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to download {url} -> {e}\")\n",
    "            continue\n",
    "        df = _parse_text_to_df(txt)\n",
    "        df[\"split_file\"] = name\n",
    "        if \"y\" in df.columns:\n",
    "            df[\"y\"] = pd.to_numeric(df[\"y\"], errors=\"coerce\")\n",
    "        dfs.append(df)\n",
    "    if not dfs:\n",
    "        raise RuntimeError(\"No dataframes downloaded/parsed successfully.\")\n",
    "    combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "    combined = combined.loc[:, ~combined.columns.str.match(r\"^Unnamed\")]\n",
    "    if \"y\" in combined.columns:\n",
    "        combined = combined.rename(columns={\"y\": \"yield\"})\n",
    "    if \"split_file\" in combined.columns:\n",
    "        combined = combined.rename(columns={\"split_file\": \"source\"})\n",
    "    if \"suzuki_index\" in combined.columns:\n",
    "        combined[\"suzuki_index\"] = pd.to_numeric(combined[\"suzuki_index\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    else:\n",
    "        candidate_idx = None\n",
    "        for c in combined.columns:\n",
    "            if combined[c].astype(str).str.match(r'^\\s*\\d+\\s*$').all():\n",
    "                candidate_idx = c\n",
    "                break\n",
    "        if candidate_idx is not None and candidate_idx != \"rxn\":\n",
    "            combined[\"suzuki_index\"] = pd.to_numeric(combined[candidate_idx], errors=\"coerce\").astype(\"Int64\")\n",
    "        else:\n",
    "            combined[\"suzuki_index\"] = pd.RangeIndex(start=0, stop=len(combined), step=1)\n",
    "    if \"rxn\" not in combined.columns:\n",
    "        rxn_col = None\n",
    "        for c in combined.columns:\n",
    "            if combined[c].astype(str).str.contains(r'[\\.\\~\\[\\]\\/\\\\\\=\\#]').any():\n",
    "                rxn_col = c\n",
    "                break\n",
    "        if rxn_col is None:\n",
    "            raise RuntimeError(\"Parsed data does not contain an 'rxn' column.\")\n",
    "        combined = combined.rename(columns={rxn_col: \"rxn\"})\n",
    "    combined[\"rxn\"] = combined[\"rxn\"].astype(str)\n",
    "    rxn_list = combined[\"rxn\"].tolist()\n",
    "    if progress:\n",
    "        rxn_list_for_jobs = list(tqdm(rxn_list, desc=\"Scheduling standardization\", leave=True))\n",
    "    else:\n",
    "        rxn_list_for_jobs = rxn_list\n",
    "    results = Parallel(n_jobs=n_jobs, prefer=\"processes\")(\n",
    "        delayed(_standardize_one)(r, remove_aam) for r in rxn_list_for_jobs\n",
    "    )\n",
    "    combined[\"rxn\"] = results\n",
    "    combined[\"R-id\"] = combined[\"suzuki_index\"].apply(lambda v: f\"Suzuki-Miyaura-{int(v)}\" if pd.notna(v) else \"\")\n",
    "    final_cols = [\"R-id\", \"rxn\"]\n",
    "    if \"yield\" in combined.columns:\n",
    "        final_cols.append(\"yield\")\n",
    "    if \"source\" in combined.columns:\n",
    "        final_cols.append(\"source\")\n",
    "    df_out = combined[final_cols].reset_index(drop=True)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_standardize(n_splits=10, n_jobs=4, remove_aam=False)\n",
    "save_df_gz(df, '../Data/property/suzuki_miyaura.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Buchwald Hartwig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://github.com/reymond-group/drfp/raw/refs/heads/main/data/Dreher_and_Doyle_input_data.xlsx\"\n",
    "r = requests.get(url, timeout=30)\n",
    "r.raise_for_status()\n",
    "buf = io.BytesIO(r.content)\n",
    "\n",
    "# list sheet names\n",
    "xls = pd.ExcelFile(buf, engine=\"openpyxl\")\n",
    "print(\"sheets:\", xls.sheet_names)\n",
    "\n",
    "# read a sheet by name (rewind buffer or re-open ExcelFile)\n",
    "df = pd.read_excel(buf, sheet_name=xls.sheet_names[0], engine=\"openpyxl\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. USPTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "_STD = None\n",
    "\n",
    "def _get_std():\n",
    "    global _STD\n",
    "    if _STD is None:\n",
    "        from synkit.Chem.Reaction.standardize import Standardize\n",
    "        _STD = Standardize()\n",
    "    return _STD\n",
    "\n",
    "def _standardize_one(rxns, remove_aam=False):\n",
    "    try:\n",
    "        std = _get_std()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Failed to import synkit.Standardize. Ensure synkit is installed.\") from e\n",
    "    try:\n",
    "        try:\n",
    "            out = std.fit(rxns, remove_aam=remove_aam)\n",
    "        except TypeError:\n",
    "            out = std.fit(rxns)\n",
    "        if out is None:\n",
    "            for attr in (\"canonical_rsmi\", \"rsmi\", \"canonical\", \"standardized\"):\n",
    "                if hasattr(std, attr):\n",
    "                    maybe = getattr(std, attr)\n",
    "                    if isinstance(maybe, str) and maybe:\n",
    "                        return maybe\n",
    "            return rxns\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            return \"|\".join(map(str, out))\n",
    "        return str(out)\n",
    "    except Exception:\n",
    "        return rxns\n",
    "\n",
    "def _find_rxn_col(df):\n",
    "    if \"rxn\" in df.columns:\n",
    "        return \"rxn\"\n",
    "    candidates = []\n",
    "    pattern = re.compile(r'[\\.\\~\\[\\]\\/\\\\\\=\\#]')  # chars typical in reaction SMILES\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            s = df[c].astype(str)\n",
    "            if s.str.contains(pattern).any():\n",
    "                candidates.append(c)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "def standardize_df_rxn(df, rxn_col=None, n_jobs=-1, remove_aam=False, progress=True):\n",
    "    if rxn_col is None:\n",
    "        rxn_col = _find_rxn_col(df)\n",
    "    if rxn_col is None:\n",
    "        raise RuntimeError(\"Could not locate an 'rxn' column. Provide rxn_col explicitly.\")\n",
    "    rxn_series = df[rxn_col].astype(str).tolist()\n",
    "    if progress:\n",
    "        rxn_for_jobs = list(tqdm(rxn_series, desc=\"Scheduling standardization\", leave=True))\n",
    "    else:\n",
    "        rxn_for_jobs = rxn_series\n",
    "    results = Parallel(n_jobs=n_jobs, prefer=\"processes\")(\n",
    "        delayed(_standardize_one)(r, remove_aam) for r in rxn_for_jobs\n",
    "    )\n",
    "    df[rxn_col] = results\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uspto_yields_above = pd.read_csv('https://github.com/reymond-group/drfp/raw/refs/heads/main/data/uspto_yields_above.csv')\n",
    "uspto_yields_above['Type'] = 'above'\n",
    "\n",
    "uspto_yields_below = pd.read_csv('https://github.com/reymond-group/drfp/raw/refs/heads/main/data/uspto_yields_below.csv')\n",
    "uspto_yields_below['Type'] = 'below'\n",
    "\n",
    "uspto_yields = pd.concat([uspto_yields_above, uspto_yields_below], axis=0, ignore_index=True)\n",
    "uspto_yields.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "uspto_yields = standardize_df_rxn(uspto_yields, n_jobs=-1, remove_aam=False, progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_gz(uspto_yields, '../Data/property/uspto_yield.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnxdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
